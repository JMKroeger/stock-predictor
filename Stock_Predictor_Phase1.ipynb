{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMKroeger/stock-predictor/blob/main/Stock_Predictor_Phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0: Environment Setup and Validation\n",
        "# Purpose: Initialize imports, configuration, logging, validate environment, and test yfinance connectivity.\n",
        "# Inputs: None\n",
        "# Outputs: pipeline.log initialized, CONFIG defined, validation results.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import sys\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "\n",
        "def retry(stop_max_attempt_number=3, wait_fixed=2):\n",
        "    def decorator(func):\n",
        "        def wrapper(*args, **kwargs):\n",
        "            for attempt in range(stop_max_attempt_number):\n",
        "                try:\n",
        "                    return func(*args, **kwargs)\n",
        "                except Exception as e:\n",
        "                    if attempt == stop_max_attempt_number - 1:\n",
        "                        raise\n",
        "                    time.sleep(wait_fixed)\n",
        "        return wrapper\n",
        "    return decorator\n",
        "\n",
        "# Configure logging\n",
        "try:\n",
        "    log_file = os.path.abspath('pipeline.log')\n",
        "    logging.basicConfig(\n",
        "        filename=log_file,\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        force=True  # Force reconfiguration if needed\n",
        "    )\n",
        "    logging.info(\"Logging initialized\")\n",
        "    print(f\"Logging initialized - pipeline.log should be at: {log_file}\")\n",
        "    # Force a write to ensure file creation\n",
        "    logging.getLogger().handlers[0].flush()\n",
        "    if os.path.exists('pipeline.log'):\n",
        "        print(\"Confirmed: pipeline.log file created successfully.\")\n",
        "    else:\n",
        "        print(\"Warning: pipeline.log file not found after setup.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up logging: {e}\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'tickers': [\n",
        "        'AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'PLTR', 'AMD', 'AMZN', 'META', 'INTC',\n",
        "        'SPY', 'QQQ', 'NFLX', 'BA', 'JPM', 'V', 'PYPL', 'DIS', 'ADBE', 'CRM',\n",
        "        'CSCO', 'WMT', 'T', 'VZ', 'CMCSA', 'PFE', 'MRK', 'KO', 'PEP'\n",
        "    ],\n",
        "    'start_date': '2015-07-04',\n",
        "    'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'telegram_token': '7779970479:AAFJFop5XrTe7_dP1iGDoGVM-bdWNyYso8E',\n",
        "    'telegram_chat_id': '1591809098',\n",
        "    'export_dir': '.',\n",
        "    'confidence_threshold': 0.8,\n",
        "    'intraday_interval': '5m',\n",
        "    'intraday_lookback_hours': 4\n",
        "}\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2)\n",
        "def test_yfinance():\n",
        "    \"\"\"Test yfinance with a single ticker and short date range.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        df = yf.download(\n",
        "            'AAPL',\n",
        "            start=(datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d'),\n",
        "            end=datetime.now().strftime('%Y-%m-%d'),\n",
        "            progress=False\n",
        "        )\n",
        "        if df.empty:\n",
        "            logging.error(\"yfinance test failed: Empty DataFrame for AAPL\")\n",
        "            raise ValueError(\"Empty DataFrame\")\n",
        "        logging.info(\"yfinance test successful\")\n",
        "        print(\"yfinance test successful\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"yfinance test failed: {e}\")\n",
        "        print(f\"yfinance test failed: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"yfinance test took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"yfinance test took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate Python and library versions.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Environment Setup and Validation\")\n",
        "    print(\"Setting up and validating environment...\")\n",
        "\n",
        "    try:\n",
        "        # Check Python version\n",
        "        if sys.version_info[:2] < (3, 6):  # Relaxed to 3.6+ for Colab compatibility\n",
        "            logging.error(\"Python 3.6 or higher required\")\n",
        "            raise ValueError(\"Python 3.6 or higher required\")\n",
        "        logging.info(f\"Python version validated: {sys.version.split()[0]}\")\n",
        "        print(f\"Python: {sys.version.split()[0]}\")\n",
        "\n",
        "        # Check library versions\n",
        "        libraries = {\n",
        "            'pandas': pd.__version__,\n",
        "            'numpy': np.__version__,\n",
        "            'yfinance': yf.__version__\n",
        "        }\n",
        "        for lib, version in libraries.items():\n",
        "            logging.info(f\"{lib}: {version}\")\n",
        "            print(f\"{lib}: {version}\")\n",
        "\n",
        "        # Test yfinance\n",
        "        test_yfinance()\n",
        "\n",
        "        # Log configuration\n",
        "        logging.info(f\"CONFIG: {len(CONFIG['tickers'])} tickers, date range {CONFIG['start_date']} to {CONFIG['end_date']}\")\n",
        "        print(\"Environment setup and validation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Environment validation failed: {e}\")\n",
        "        print(f\"Error in environment validation: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Environment setup and validation took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Setup and validation took {elapsed_time:.2f} seconds\")\n",
        "        logging.getLogger().handlers[0].flush()  # Flush again to ensure write\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    validate_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugwxR-4Wvm5A",
        "outputId": "6ed52d74-c07f-4f17-e9aa-a7a27f8006a5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logging initialized - pipeline.log should be at: /content/pipeline.log\n",
            "Confirmed: pipeline.log file created successfully.\n",
            "Setting up and validating environment...\n",
            "Python: 3.11.13\n",
            "pandas: 2.2.2\n",
            "numpy: 2.0.2\n",
            "yfinance: 0.2.65\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-461882779.py:71: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yfinance test successful\n",
            "yfinance test took 0.39 seconds\n",
            "Environment setup and validation complete\n",
            "Setup and validation took 0.40 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Fetch and Transform Data\n",
        "# Purpose: Fetch historical stock data using yfinance, cache to SQLite, save to raw_stock_data.csv.\n",
        "# Inputs: CONFIG from Cell 0, yfinance API.\n",
        "# Outputs: stock_data.db, raw_stock_data.csv, logs to pipeline.log.\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import sqlite3\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2)\n",
        "def fetch_historical_data(ticker):\n",
        "    \"\"\"Fetch historical data with retry logic.\"\"\"\n",
        "    try:\n",
        "        df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
        "        if df.empty:\n",
        "            logging.warning(f\"No data fetched for {ticker}\")\n",
        "            return None\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "        df['Ticker'] = ticker\n",
        "        df['Date'] = df.index\n",
        "        df = df.reset_index(drop=True)\n",
        "        col_map = {\n",
        "            'Open': 'Open', 'High': 'High', 'Low': 'Low', 'Close': 'Close', 'Volume': 'Volume',\n",
        "            'Adj Close': 'Adj_Close', 'Adjusted Close': 'Adj_Close'\n",
        "        }\n",
        "        df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n",
        "        required_cols = ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj_Close']\n",
        "        for col in required_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = df['Close'] if col == 'Adj_Close' else None\n",
        "        return df[required_cols]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_sqlite(df, conn):\n",
        "    \"\"\"Save DataFrame to SQLite with consistent schema.\"\"\"\n",
        "    try:\n",
        "        df.to_sql('historical_data', conn, if_exists='append', index=False)\n",
        "        logging.debug(f\"Data saved to SQLite for {df['Ticker'].iloc[0]}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving to SQLite: {e}\")\n",
        "        raise\n",
        "\n",
        "def initialize_sqlite():\n",
        "    \"\"\"Initialize SQLite database with historical_data table.\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect('stock_data.db')\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"DROP TABLE IF EXISTS historical_data\")\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS historical_data (\n",
        "                Date TEXT,\n",
        "                Ticker TEXT,\n",
        "                Open REAL,\n",
        "                High REAL,\n",
        "                Low REAL,\n",
        "                Close REAL,\n",
        "                Volume INTEGER,\n",
        "                Adj_Close REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing SQLite: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fetch and store historical data.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 1: Data fetching\")\n",
        "    print(\"Fetching historical data...\")\n",
        "\n",
        "    try:\n",
        "        conn = initialize_sqlite()\n",
        "        all_data = []\n",
        "        for ticker in CONFIG['tickers']:\n",
        "            df = fetch_historical_data(ticker)\n",
        "            if df is not None:\n",
        "                all_data.append(df)\n",
        "                save_to_sqlite(df, conn)\n",
        "                logging.debug(f\"Fetched and saved data for {ticker}\")\n",
        "            else:\n",
        "                logging.warning(f\"Failed to fetch data for {ticker}\")\n",
        "        conn.close()\n",
        "        if all_data:\n",
        "            combined_df = pd.concat(all_data, ignore_index=True)\n",
        "            combined_df.to_csv('raw_stock_data.csv', index=False)\n",
        "            logging.info(\"Saved data to raw_stock_data.csv\")\n",
        "            print(\"Data saved to raw_stock_data.csv\")\n",
        "        else:\n",
        "            logging.warning(\"No data fetched for any ticker\")\n",
        "            print(\"No data fetched\")\n",
        "        logging.info(\"Cell 1: Data fetching successful\")\n",
        "        print(\"Data fetching complete\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 1: Failed: {e}\")\n",
        "        print(f\"Error in Cell 1: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 1 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Data fetching took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy7hL8BYv_yI",
        "outputId": "2a8eee77-27df-4283-93dd-52469da74d9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching historical data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-2-3538072553.py:24: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to raw_stock_data.csv\n",
            "Data fetching complete\n",
            "Data fetching took 19.22 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Calculate Technical Indicators\n",
        "# Purpose: Compute technical indicators for historical data to enhance ML predictions.\n",
        "# Inputs: raw_stock_data.csv, stock_data.db.\n",
        "# Outputs: processed_stock_data.csv, logs to pipeline.log.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import sqlite3\n",
        "import time\n",
        "import os\n",
        "import yfinance as yf\n",
        "from textblob import TextBlob\n",
        "import json\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def rsi(close, length=14):\n",
        "    delta = close.diff()\n",
        "    gain = delta.where(delta > 0, 0)\n",
        "    loss = -delta.where(delta < 0, 0)\n",
        "    avg_gain = gain.ewm(com=length-1, min_periods=length).mean()\n",
        "    avg_loss = loss.ewm(com=length-1, min_periods=length).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    return 100 - 100 / (1 + rs)\n",
        "\n",
        "def stoch(high, low, close, k=14, smooth_k=3):\n",
        "    l14 = low.rolling(k).min()\n",
        "    h14 = high.rolling(k).max()\n",
        "    per_k = 100 * (close - l14) / (h14 - l14)\n",
        "    per_k = per_k.rolling(smooth_k).mean()\n",
        "    return per_k\n",
        "\n",
        "def mfi(high, low, close, volume, length=14):\n",
        "    tp = (high + low + close) / 3\n",
        "    mf = tp * volume\n",
        "    diff = tp.diff()\n",
        "    pos_mf = mf.where(diff > 0, 0).ewm(com=length-1, min_periods=length).mean()\n",
        "    neg_mf = -mf.where(diff < 0, 0).ewm(com=length-1, min_periods=length).mean()\n",
        "    mfr = pos_mf / neg_mf\n",
        "    return 100 - 100 / (1 + mfr)\n",
        "\n",
        "def macd(close, fast=12, slow=26, signal=9):\n",
        "    ema_fast = close.ewm(span=fast, adjust=False).mean()\n",
        "    ema_slow = close.ewm(span=slow, adjust=False).mean()\n",
        "    macd_line = ema_fast - ema_slow\n",
        "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
        "    return macd_line\n",
        "\n",
        "def adx(high, low, close, length=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    atr = tr.ewm(com=length-1, min_periods=length).mean()\n",
        "    up = (high - high.shift()).clip(lower=0)\n",
        "    down = (low.shift() - low).clip(lower=0)\n",
        "    pos_di = 100 * (up.ewm(com=length-1, min_periods=length).mean() / atr)\n",
        "    neg_di = 100 * (down.ewm(com=length-1, min_periods=length).mean() / atr)\n",
        "    dx = 100 * abs(pos_di - neg_di) / (pos_di + neg_di)\n",
        "    return dx.ewm(com=length-1, min_periods=length).mean()\n",
        "\n",
        "def ichimoku_a(high, low, close, tenkan=9, kijun=26, senkou=52):\n",
        "    tenkan_max = high.rolling(tenkan).max()\n",
        "    tenkan_min = low.rolling(tenkan).min()\n",
        "    tenkan_sen = (tenkan_max + tenkan_min) / 2\n",
        "    kijun_max = high.rolling(kijun).max()\n",
        "    kijun_min = low.rolling(kijun).min()\n",
        "    kijun_sen = (kijun_max + kijun_min) / 2\n",
        "    senkou_a = (tenkan_sen + kijun_sen) / 2\n",
        "    return senkou_a.shift(kijun)\n",
        "\n",
        "def supertrend(high, low, close, length=10, multiplier=3):\n",
        "    atr = atr_func(high, low, close, length)\n",
        "    hl2 = (high + low) / 2\n",
        "    upper = hl2 + multiplier * atr\n",
        "    lower = hl2 - multiplier * atr\n",
        "    upper = upper.where((close.shift() > upper.shift()) | upper.shift().isna(), upper.shift())\n",
        "    lower = lower.where((close.shift() < lower.shift()) | lower.shift().isna(), lower.shift())\n",
        "    trend = pd.Series(0.0, index=close.index)\n",
        "    for i in range(1, len(close)):\n",
        "        if close.iloc[i] > upper.iloc[i-1]:\n",
        "            trend.iloc[i] = lower.iloc[i]\n",
        "        elif close.iloc[i] < lower.iloc[i-1]:\n",
        "            trend.iloc[i] = upper.iloc[i]\n",
        "        else:\n",
        "            trend.iloc[i] = trend.iloc[i-1]\n",
        "            if trend.iloc[i-1] == upper.iloc[i-1] and close.iloc[i] < lower.iloc[i]:\n",
        "                trend.iloc[i] = upper.iloc[i]\n",
        "            elif trend.iloc[i-1] == lower.iloc[i-1] and close.iloc[i] > upper.iloc[i]:\n",
        "                trend.iloc[i] = lower.iloc[i]\n",
        "    return trend\n",
        "\n",
        "def bbands(close, length=20, std=2):\n",
        "    ma = close.rolling(length).mean()\n",
        "    sd = close.rolling(length).std()\n",
        "    upper = ma + std * sd\n",
        "    lower = ma - std * sd\n",
        "    return upper, lower\n",
        "\n",
        "def atr_func(high, low, close, length=14):\n",
        "    tr1 = high - low\n",
        "    tr2 = abs(high - close.shift())\n",
        "    tr3 = abs(low - close.shift())\n",
        "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
        "    return tr.ewm(com=length-1, min_periods=length).mean()\n",
        "\n",
        "def kc(high, low, close, length=20, scalar=2, mamode=\"ema\"):\n",
        "    if mamode == \"ema\":\n",
        "        ma = close.ewm(span=length, adjust=False).mean()\n",
        "    else:\n",
        "        ma = close.rolling(length).mean()\n",
        "    atr = atr_func(high, low, close, 10)\n",
        "    upper = ma + scalar * atr\n",
        "    lower = ma - scalar * atr\n",
        "    return upper, lower\n",
        "\n",
        "def vwap(high, low, close, volume):\n",
        "    tp = (high + low + close) / 3\n",
        "    return (tp * volume).cumsum() / volume.cumsum()\n",
        "\n",
        "def chaikin_ad(high, low, close, volume):\n",
        "    mfm = ((close - low) - (high - close)) / (high - low)\n",
        "    mfv = mfm * volume\n",
        "    return mfv.cumsum()\n",
        "\n",
        "def cmf(high, low, close, volume, length=20):\n",
        "    ad = chaikin_ad(high, low, close, volume)\n",
        "    return ad.rolling(length).sum() / volume.rolling(length).sum()\n",
        "\n",
        "def obv(close, volume):\n",
        "    diff = close.diff()\n",
        "    sign = diff.gt(0).astype(int) - diff.lt(0).astype(int)\n",
        "    return (sign * volume).cumsum()\n",
        "\n",
        "def force_index(close, volume, length=13):\n",
        "    fi = close.diff() * volume\n",
        "    return fi.ewm(span=length, adjust=False).mean()\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    \"\"\"Calculate technical indicators for a single ticker using custom functions.\"\"\"\n",
        "    try:\n",
        "        # Momentum\n",
        "        df['RSI'] = rsi(df['Close'])\n",
        "        df['Stoch_K'] = stoch(df['High'], df['Low'], df['Close'])\n",
        "        df['MFI'] = mfi(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "\n",
        "        # Trend\n",
        "        df['MACD'] = macd(df['Close'])\n",
        "        df['ADX'] = adx(df['High'], df['Low'], df['Close'])\n",
        "        df['Ichimoku_A'] = ichimoku_a(df['High'], df['Low'], df['Close'])\n",
        "        df['Supertrend'] = supertrend(df['High'], df['Low'], df['Close'])\n",
        "\n",
        "        # Volatility\n",
        "        df['BB_Upper'], df['BB_Lower'] = bbands(df['Close'])\n",
        "        df['ATR'] = atr_func(df['High'], df['Low'], df['Close'])\n",
        "        df['KC_Upper'], df['KC_Lower'] = kc(df['High'], df['Low'], df['Close'])\n",
        "\n",
        "        # Volume\n",
        "        df['VWAP'] = vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        df['AD'] = chaikin_ad(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        df['CMF'] = cmf(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        df['OBV'] = obv(df['Close'], df['Volume'])\n",
        "        df['Volume_Osc'] = force_index(df['Close'], df['Volume'])\n",
        "\n",
        "        return df.fillna(0)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating indicators: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_sentiment_score(ticker):\n",
        "    \"\"\"Fetch recent news from yfinance and compute average sentiment using TextBlob.\"\"\"\n",
        "    try:\n",
        "        t = yf.Ticker(ticker)\n",
        "        news = t.news\n",
        "        if not news:\n",
        "            logging.debug(f\"No news for {ticker}, using neutral sentiment\")\n",
        "            return 0.0\n",
        "        scores = []\n",
        "        for article in news:\n",
        "            text = article.get('title') or article.get('summary', '')\n",
        "            if text:\n",
        "                scores.append(TextBlob(text).sentiment.polarity)\n",
        "        if not scores:\n",
        "            logging.debug(f\"No valid text for sentiment in news for {ticker}\")\n",
        "            return 0.0\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Sentiment analysis failed for {ticker}: {e}\")\n",
        "        return 0.0\n",
        "\n",
        "def main():\n",
        "    \"\"\"Calculate indicators and save results.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 2: Indicator calculation\")\n",
        "    print(\"Calculating indicators...\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists('raw_stock_data.csv'):\n",
        "            logging.error(\"Input file raw_stock_data.csv not found\")\n",
        "            raise FileNotFoundError(\"raw_stock_data.csv not found\")\n",
        "\n",
        "        df = pd.read_csv('raw_stock_data.csv')\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        sentiment_dict = {ticker: get_sentiment_score(ticker) for ticker in df['Ticker'].unique()}\n",
        "        df['Sentiment'] = df['Ticker'].map(sentiment_dict)\n",
        "        with open('sentiment_scores.json', 'w') as f:\n",
        "            json.dump(sentiment_dict, f)\n",
        "\n",
        "        conn = sqlite3.connect('stock_data.db')\n",
        "\n",
        "        all_data = []\n",
        "        for ticker in df['Ticker'].unique():\n",
        "            ticker_df = df[df['Ticker'] == ticker].sort_values('Date').set_index('Date')\n",
        "            ticker_df = calculate_indicators(ticker_df)\n",
        "            if ticker_df is not None:\n",
        "                ticker_df['Ticker'] = ticker\n",
        "                all_data.append(ticker_df.reset_index())\n",
        "\n",
        "        if all_data:\n",
        "            processed_df = pd.concat(all_data, ignore_index=True)\n",
        "            processed_df.to_csv('processed_stock_data.csv', index=False)\n",
        "            processed_df.to_sql('processed_data', conn, if_exists='replace', index=False)\n",
        "            logging.info(\"Saved processed data to processed_stock_data.csv and SQLite\")\n",
        "            print(\"Processed data saved\")\n",
        "        else:\n",
        "            logging.warning(\"No processed data generated\")\n",
        "            print(\"No processed data\")\n",
        "\n",
        "        conn.close()\n",
        "        logging.info(\"Cell 2: Indicator calculation successful\")\n",
        "        print(\"Indicator calculation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 2: Failed: {e}\")\n",
        "        print(f\"Error in Cell 2: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 2 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Indicator calculation took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlWX08uvwQXc",
        "outputId": "1f3cc456-8296-4481-c4b2-91fd56c0f4ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating indicators...\n",
            "Processed data saved\n",
            "Indicator calculation complete\n",
            "Indicator calculation took 17.16 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: One-time Historical Indicator Weighting\n",
        "# Purpose: Drop and recreate indicator_weights table, compute feature importance for historical data using Random Forest, store weights in SQLite.\n",
        "# Inputs: Historical data from yfinance, indicators from Cell 2.\n",
        "# Outputs: indicator_weights.db updated with feature importance.\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(filename='pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Define tickers and date range\n",
        "TICKERS = CONFIG['tickers']\n",
        "START_DATE = CONFIG['start_date']\n",
        "END_DATE = CONFIG['end_date']\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "def fetch_historical_data(ticker):\n",
        "    \"\"\"Fetch historical data with retry logic.\"\"\"\n",
        "    try:\n",
        "        df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
        "        if df.empty:\n",
        "            logging.warning(f\"No data fetched for {ticker}\")\n",
        "            return None\n",
        "        df['Ticker'] = ticker\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data for {ticker}: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    \"\"\"Calculate technical indicators using custom functions, individually with error handling.\"\"\"\n",
        "    indicators = {\n",
        "        'RSI': rsi,\n",
        "        'MACD': macd,\n",
        "        'BB_Upper': lambda x: bbands(x)[0],  # Assuming bbands returns (upper, lower)\n",
        "        'ATR': lambda x: atr_func(df['High'], df['Low'], df['Close']),\n",
        "        'VWAP': lambda x: vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "    }\n",
        "    for ind, func in indicators.items():\n",
        "        try:\n",
        "            if ind == 'BB_Upper':\n",
        "                df[ind], _ = func(df['Close'])\n",
        "            else:\n",
        "                df[ind] = func(df['Close'])\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating {ind}: {str(e)}\")\n",
        "            df[ind] = np.nan\n",
        "    return df\n",
        "\n",
        "def compute_feature_importance(df):\n",
        "    \"\"\"Compute feature importance using Random Forest.\"\"\"\n",
        "    features = ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
        "    try:\n",
        "        if df is None or df.empty:\n",
        "            logging.warning(\"No data for feature importance\")\n",
        "            return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "        # Filter to available features\n",
        "        available_features = [f for f in features if f in df.columns]\n",
        "        if not available_features:\n",
        "            logging.warning(\"No available features for importance\")\n",
        "            return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "        df = df.dropna(subset=available_features + ['Close'])\n",
        "        if len(df) < 10:\n",
        "            logging.warning(\"Too few samples for feature importance\")\n",
        "            return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "        df['Target'] = (df['Close'].shift(-1) > df['Close'] * 1.01).astype(int)\n",
        "        df = df.dropna(subset=['Target'])\n",
        "        if df.empty:\n",
        "            logging.warning(\"Empty DF after Target dropna\")\n",
        "            return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "        if len(df['Target'].unique()) < 2:\n",
        "            logging.warning(\"Only one class in Target, using default importance\")\n",
        "            return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "        X = df[available_features]\n",
        "        # Handle inf/NaN\n",
        "        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "        y = df['Target']\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(X, y)\n",
        "        importance = pd.DataFrame({\n",
        "            'Feature': available_features,\n",
        "            'Importance': model.feature_importances_\n",
        "        })\n",
        "        # Pad missing features with 0 importance\n",
        "        missing = set(features) - set(available_features)\n",
        "        if missing:\n",
        "            missing_df = pd.DataFrame({'Feature': list(missing), 'Importance': [0.0] * len(missing)})\n",
        "            importance = pd.concat([importance, missing_df])\n",
        "        return importance\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error computing feature importance: {str(e)}\")\n",
        "        return pd.DataFrame({'Feature': features, 'Importance': [0.2] * len(features)})\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process tickers and store weights.\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect('indicator_weights.db')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        cursor.execute(\"DROP TABLE IF EXISTS indicator_weights\")\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE indicator_weights (\n",
        "                ticker TEXT,\n",
        "                feature TEXT,\n",
        "                importance REAL,\n",
        "                date TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        for ticker in TICKERS:\n",
        "            logging.info(f\"Processing {ticker} for indicator weighting\")\n",
        "\n",
        "            df = fetch_historical_data(ticker)\n",
        "            if df is None:\n",
        "                continue\n",
        "\n",
        "            df = calculate_indicators(df)\n",
        "            if df is None:  # Though now less likely with individual tries\n",
        "                continue\n",
        "\n",
        "            importance = compute_feature_importance(df)\n",
        "            # importance always returned now\n",
        "\n",
        "            current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "            for _, row in importance.iterrows():\n",
        "                cursor.execute(\"\"\"\n",
        "                    INSERT INTO indicator_weights (ticker, feature, importance, date)\n",
        "                    VALUES (?, ?, ?, ?)\n",
        "                \"\"\", (ticker, row['Feature'], row['Importance'], current_date))\n",
        "\n",
        "            conn.commit()\n",
        "            logging.info(f\"Stored weights for {ticker}\")\n",
        "\n",
        "        conn.close()\n",
        "        logging.info(\"Cell 3 completed successfully\")\n",
        "\n",
        "        print(\"Indicator weights calculated and stored in indicator_weights.db\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 3 failed: {str(e)}\")\n",
        "        print(f\"Error in Cell 3: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OuoHurEwQxT",
        "outputId": "4adb7230-0c45-4a0d-d674-3c36a9d2289b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Training and Feature Selection\n",
        "# Purpose: Train ML models (RF, XGBoost) using Optuna for hyperparams, select features based on weights, generate predictions.\n",
        "# Inputs: processed_stock_data.csv, indicator_weights.db.\n",
        "# Outputs: model_rf.pkl, model_xgb.pkl, scaler.pkl, selected_features.pkl, predictions.csv.\n",
        "\n",
        "!pip install -q optuna joblib xgboost\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import sqlite3\n",
        "import time\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from xgboost import XGBClassifier\n",
        "import optuna\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def train_model(data):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df = df.dropna(subset=['Close', 'Ticker'])\n",
        "\n",
        "        sentiment_path = f\"{CONFIG['export_dir']}/sentiment_scores.json\"\n",
        "        if os.path.exists(sentiment_path):\n",
        "            with open(sentiment_path, 'r') as f:\n",
        "                sentiment_scores = json.load(f)\n",
        "            df['Sentiment'] = df['Ticker'].map(sentiment_scores).fillna(0.5)\n",
        "        else:\n",
        "            logging.warning(\"Cell 4: No sentiment scores found, using neutral values\")\n",
        "            df['Sentiment'] = 0.5\n",
        "\n",
        "        features = [\n",
        "            'RSI', 'Stoch_K', 'MFI', 'MACD', 'ADX', 'Ichimoku_A', 'Supertrend',\n",
        "            'BB_Upper', 'BB_Lower', 'ATR', 'KC_Upper', 'KC_Lower', 'VWAP', 'AD', 'CMF', 'OBV', 'Volume_Osc', 'Sentiment'\n",
        "        ]\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/indicator_weights.db\")\n",
        "        weights_df = pd.read_sql_query(\"SELECT * FROM indicator_weights\", conn)  # Adjusted query as 'Type' may not exist\n",
        "        conn.close()\n",
        "        # Aggregate importance by feature, mean across tickers\n",
        "        weights_df = weights_df.groupby('feature')['importance'].mean().reset_index()\n",
        "        weights = dict(zip(weights_df['feature'], weights_df['importance']))\n",
        "        weighted_features = sorted(\n",
        "            [f for f in features if f in weights],\n",
        "            key=lambda x: weights.get(x, 0),\n",
        "            reverse=True\n",
        "        )[:10]\n",
        "        selected_features = [col for col in weighted_features if col in df.columns]  # Ensure in df\n",
        "        logging.info(f\"Cell 4: Using {len(selected_features)} weighted features: {selected_features}\")\n",
        "\n",
        "        for file in ['model_rf.pkl', 'model_xgb.pkl', 'scaler.pkl', 'selected_features.pkl']:\n",
        "            file_path = f\"{CONFIG['export_dir']}/{file}\"\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                logging.info(f\"Cell 4: Cleared {file_path}\")\n",
        "\n",
        "        if not selected_features:\n",
        "            selected_features = [col for col in features if col in df.columns]\n",
        "            logging.warning(f\"Cell 4: No weighted features, using all available: {selected_features}\")\n",
        "\n",
        "        missing_features = [f for f in selected_features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            logging.warning(f\"Cell 4: Missing features: {missing_features}, filling with zeros\")\n",
        "            for f in missing_features:\n",
        "                df[f] = 0\n",
        "\n",
        "        df['Volume'] = np.log1p(df['Volume'].clip(lower=1))\n",
        "        if 'VWAP' in df.columns:\n",
        "            df['VWAP'] = df['VWAP'].clip(lower=0, upper=df['Close'].max() * 2)\n",
        "        else:\n",
        "            df['VWAP'] = df['Close']\n",
        "        if 'ATR' in df.columns:\n",
        "            df['ATR'] = df['ATR'].clip(lower=0, upper=0.1)\n",
        "        else:\n",
        "            df['ATR'] = 1.0\n",
        "\n",
        "        logging.info(f\"Cell 4: Features available: {selected_features}\")\n",
        "        print(f\"Debug: Features available: {selected_features}\")\n",
        "        print(f\"Debug: Data shape before training: {df.shape}\")\n",
        "\n",
        "        volatility = df.groupby('Ticker')['Close'].pct_change().rolling(window=5).std().groupby(df['Ticker']).last()\n",
        "        df['Next_Close'] = df.groupby('Ticker')['Close'].shift(-1)\n",
        "        df['Target'] = df.apply(\n",
        "            lambda row: 1 if pd.notna(row['Next_Close']) and row['Next_Close'] > row['Close'] * (1 + volatility.get(row['Ticker'], 0.003) * 0.15) else 0,\n",
        "            axis=1\n",
        "        )\n",
        "        df = df.drop(columns=['Next_Close']).dropna(subset=['Target', 'Close'] + selected_features)\n",
        "\n",
        "        latest_date = df['Date'].max()\n",
        "        train_data = df[df['Date'] < latest_date]\n",
        "        pred_data = df[df['Date'] == latest_date]\n",
        "\n",
        "        if train_data.empty or pred_data.empty:\n",
        "            raise ValueError(\"Cell 4: Insufficient data for training or prediction\")\n",
        "\n",
        "        X_train = train_data[selected_features]\n",
        "        y_train = train_data['Target']\n",
        "        X_train = X_train.fillna(0)\n",
        "        y_train = y_train.fillna(0).astype(int)\n",
        "\n",
        "        logging.info(f\"Cell 4: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        logging.info(f\"Cell 4: Target distribution: {y_train.value_counts().to_dict()}\")\n",
        "        print(f\"Debug: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        print(f\"Debug: Target distribution: {y_train.value_counts()}\")\n",
        "\n",
        "        def train_split(train_idx, test_idx, X_train, y_train, selected_features):\n",
        "            X_train_split, X_test_split = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
        "            y_train_split, y_test_split = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train_split)\n",
        "            X_test_scaled = scaler.transform(X_test_split)\n",
        "\n",
        "            # Subsample for tuning: last 20000 rows for time series\n",
        "            sample_size = 20000\n",
        "            if len(X_train_scaled) > sample_size:\n",
        "                X_tune = X_train_scaled[-sample_size:]\n",
        "                y_tune = y_train_split.iloc[-sample_size:]\n",
        "            else:\n",
        "                X_tune = X_train_scaled\n",
        "                y_tune = y_train_split\n",
        "\n",
        "            def rf_objective(trial):\n",
        "                rf_params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
        "                    'min_samples_split': trial.suggest_int('min_samples_split', 5, 10),\n",
        "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 5)\n",
        "                }\n",
        "                rf_model = RandomForestClassifier(**rf_params, random_state=42, class_weight='balanced')\n",
        "                rf_model.fit(X_tune, y_tune)\n",
        "                rf_pred = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "                return precision_score(y_test_split, (rf_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            def xgb_objective(trial):\n",
        "                xgb_params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
        "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05)\n",
        "                }\n",
        "                xgb_model = XGBClassifier(**xgb_params, random_state=42, scale_pos_weight=max(1, (y_tune==0).sum() / (y_tune==1).sum()))\n",
        "                xgb_model.fit(X_tune, y_tune)\n",
        "                xgb_pred = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "                return precision_score(y_test_split, (xgb_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            rf_study = optuna.create_study(direction='maximize')\n",
        "            rf_study.optimize(rf_objective, n_trials=5)\n",
        "            rf_model = RandomForestClassifier(**rf_study.best_params, random_state=42, class_weight='balanced')\n",
        "            rf_model.fit(X_train_scaled, y_train_split)\n",
        "            rf_pred = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "            rf_score = precision_score(y_test_split, (rf_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            xgb_study = optuna.create_study(direction='maximize')\n",
        "            xgb_study.optimize(xgb_objective, n_trials=5)\n",
        "            xgb_model = XGBClassifier(**xgb_study.best_params, random_state=42, scale_pos_weight=max(1, (y_train_split==0).sum() / (y_train_split==1).sum()))\n",
        "            xgb_model.fit(X_train_scaled, y_train_split)\n",
        "            xgb_pred = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "            xgb_score = precision_score(y_test_split, (xgb_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            return rf_score, xgb_score, rf_model, xgb_model, scaler\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(train_split)(train_idx, test_idx, X_train, y_train, selected_features)\n",
        "            for train_idx, test_idx in tscv.split(X_train)\n",
        "        )\n",
        "        rf_scores = [r[0] for r in results]\n",
        "        xgb_scores = [r[1] for r in results]\n",
        "        rf_model = results[-1][2]\n",
        "        xgb_model = results[-1][3]\n",
        "        scaler = results[-1][4]\n",
        "\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_pred_scaled = scaler.transform(pred_data[selected_features].fillna(0))\n",
        "\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "        rf_pred = rf_model.predict_proba(X_pred_scaled)[:, 1]\n",
        "        xgb_pred = xgb_model.predict_proba(X_pred_scaled)[:, 1]\n",
        "        confidence = (rf_pred + xgb_pred) / 2\n",
        "\n",
        "        predictions_df = pred_data[['Ticker', 'Date', 'Close']].copy()\n",
        "        predictions_df['Confidence'] = confidence\n",
        "        predictions_df['Prediction'] = (confidence > 0.5).astype(int)\n",
        "        predictions_df = predictions_df.sort_values('Confidence', ascending=False)\n",
        "\n",
        "        rf_importance = pd.DataFrame({\n",
        "            'Feature': selected_features,\n",
        "            'Importance': rf_model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        logging.info(f\"Cell 4: RF Feature Importance (Top 5): {rf_importance.head().to_dict()}\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/model_rf.pkl\", 'wb') as f:\n",
        "            pickle.dump(rf_model, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/model_rf.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/model_xgb.pkl\", 'wb') as f:\n",
        "            pickle.dump(xgb_model, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/model_xgb.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/scaler.pkl\", 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/scaler.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/selected_features.pkl\", 'wb') as f:\n",
        "            pickle.dump(selected_features, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/selected_features.pkl\")\n",
        "        predictions_df.to_csv(f\"{CONFIG['export_dir']}/predictions.csv\", index=False)\n",
        "\n",
        "        avg_cv_accuracy = (np.mean(rf_scores) + np.mean(xgb_scores)) / 2 * 100\n",
        "        logging.info(f\"Cell 4: Model trained, Avg CV Accuracy: {avg_cv_accuracy:.2f}%, Predictions shape: {predictions_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 4: Model trained, Avg CV Accuracy: {avg_cv_accuracy:.2f}%, Predictions shape: {predictions_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return scaler, selected_features, predictions_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 4: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "scaler, selected_features, predictions_df = train_model(processed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iROxTnHHwRBM",
        "outputId": "8ac36fd1-55e6-4fe3-c1e1-be3e9f448c39"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Debug: Features available: ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
            "Debug: Data shape before training: (71731, 26)\n",
            "Debug: X_train shape: (71702, 5), y_train shape: (71702,)\n",
            "Debug: Target distribution: Target\n",
            "0    38463\n",
            "1    33239\n",
            "Name: count, dtype: int64\n",
            "Cell 4: Model trained, Avg CV Accuracy: 47.36%, Predictions shape: (29, 5), Time: 77.02s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Generate Trade Candidates\n",
        "# Purpose: Generate trade candidates based on predictions, excluding recent losses.\n",
        "# Inputs: predictions.csv, loss_tracker.db.\n",
        "# Outputs: trade_candidates.csv.\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "def generate_trade_candidates(predictions_df, confidence_threshold=CONFIG['confidence_threshold']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if predictions_df.empty:\n",
        "            raise ValueError(\"Cell 5: Empty predictions dataframe\")\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"CREATE TABLE IF NOT EXISTS loss_tracker (Ticker TEXT, Loss_Date TEXT)\")\n",
        "        conn.commit()\n",
        "        loss_data = pd.read_sql(\"SELECT * FROM loss_tracker\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        current_date = datetime.now()\n",
        "        excluded_tickers = []\n",
        "        for _, row in loss_data.iterrows():\n",
        "            loss_date = pd.to_datetime(row['Loss_Date'])\n",
        "            if (current_date - loss_date).days < 35:\n",
        "                excluded_tickers.append(row['Ticker'])\n",
        "        excluded_tickers = list(set(excluded_tickers))\n",
        "\n",
        "        df = predictions_df.copy()\n",
        "        df = df[~df['Ticker'].isin(excluded_tickers)]\n",
        "        volatility = df.groupby('Ticker')['Close'].pct_change().rolling(window=5).std().groupby(df['Ticker']).last()\n",
        "        df['Volatility'] = df['Ticker'].map(volatility).fillna(1.0)\n",
        "\n",
        "        candidates = df[df['Confidence'] >= confidence_threshold][['Ticker', 'Date', 'Close', 'Confidence', 'Prediction', 'Volatility']]\n",
        "        candidates = candidates.sort_values('Confidence', ascending=False)\n",
        "\n",
        "        candidates.to_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\", index=False)\n",
        "        logging.info(f\"Cell 5: Confidence threshold: {confidence_threshold}, Volatility: {candidates['Volatility'].mean():.4f}\")\n",
        "        logging.info(f\"Cell 5: Confidence stats: {candidates['Confidence'].describe().to_dict()}\")\n",
        "        logging.info(f\"Cell 5: Generated {len(candidates)} trade candidates, Excluded tickers: {excluded_tickers}\")\n",
        "        logging.info(f\"Cell 5: Trade candidates generated, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 5: Trade candidates generated, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return candidates\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 5: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "predictions_df = pd.read_csv(f\"{CONFIG['export_dir']}/predictions.csv\")\n",
        "trade_candidates_df = generate_trade_candidates(predictions_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCbASvclwRTY",
        "outputId": "676a785f-a8d2-40c1-8a02-2caf656bd339"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 5: Trade candidates generated, Shape: (0, 6), Time: 0.01s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Telegram Alerts\n",
        "# Purpose: Send Telegram alerts for trade candidates.\n",
        "# Inputs: trade_candidates.csv.\n",
        "# Outputs: Telegram messages sent, logs to pipeline.log.\n",
        "\n",
        "import requests\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "def send_telegram_alert(candidates_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if candidates_df.empty:\n",
        "            logging.warning(\"Cell 6: No candidates for Telegram alert\")\n",
        "            print(\"Cell 6: No candidates for Telegram alert\")\n",
        "            return\n",
        "\n",
        "        bot_token = CONFIG['telegram_token']\n",
        "        chat_id = CONFIG['telegram_chat_id']\n",
        "        candidates_df['Type'] = 'Buy'  # Assume all are buy for now\n",
        "        for _, row in candidates_df.iterrows():\n",
        "            if row['Type'] == 'Buy':\n",
        "                message = (\n",
        "                    f\" Trade Alert \\n\"\n",
        "                    f\"Ticker: {row['Ticker']}\\n\"\n",
        "                    f\"Date: {row['Date']}\\n\"\n",
        "                    f\"Action: Buy\\n\"\n",
        "                    f\"Entry Price: ${row['Close']:.2f}\\n\"\n",
        "                    f\"Confidence: {row['Confidence']:.2%}\\n\"\n",
        "                    f\"Volatility: {row['Volatility']:.4f}\"\n",
        "                )\n",
        "                url = f\"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={message}\"\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                logging.info(f\"Cell 6: Telegram alert sent for {row['Ticker']}.\")\n",
        "                print(f\"Cell 6: Telegram alert sent for {row['Ticker']}.\")\n",
        "                time.sleep(0.2)\n",
        "        logging.info(f\"Cell 6: Telegram alerts completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 6: Telegram alerts completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 6: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "send_telegram_alert(trade_candidates_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovhzx9rewRlO",
        "outputId": "e03b018d-eb62-4eac-80eb-d8b845378b75"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 6: No candidates for Telegram alert\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Backtest Strategy\n",
        "# Purpose: Backtest the strategy on trade candidates.\n",
        "# Inputs: processed_stock_data.csv, trade_candidates.csv.\n",
        "# Outputs: backtest_results.csv, updates loss_tracker.db.\n",
        "\n",
        "import pandas as pd\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def backtest_strategy(data, trade_candidates, target_multiplier=0.8, stop_loss_multiplier=2.0):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if trade_candidates.empty or data.empty:\n",
        "            logging.warning(\"Cell 7: Empty trade candidates or data, skipping backtest\")\n",
        "            print(\"Cell 7: No trades to backtest\")\n",
        "            return pd.DataFrame()  # Return empty DF instead of raising error\n",
        "\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        candidates = trade_candidates.copy()\n",
        "        candidates['Date'] = pd.to_datetime(candidates['Date'])\n",
        "\n",
        "        results = []\n",
        "        for _, candidate in candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            entry_date = candidate['Date']\n",
        "            entry_price = candidate['Close']\n",
        "\n",
        "            start_date = entry_date - timedelta(days=30)\n",
        "            end_date = entry_date\n",
        "            ticker_data = df[(df['Ticker'] == ticker) & (df['Date'] >= start_date) & (df['Date'] <= end_date)].sort_values('Date')\n",
        "\n",
        "            if len(ticker_data) < 14:\n",
        "                logging.warning(f\"Cell 7: Insufficient data for {ticker} from {start_date} to {end_date}, skipping\")\n",
        "                continue\n",
        "\n",
        "            atr = ticker_data['ATR'].iloc[-1] if 'ATR' in ticker_data.columns else 1.0\n",
        "            atr = atr if pd.notna(atr) else 1.0\n",
        "\n",
        "            target_price = entry_price + atr * target_multiplier\n",
        "            stop_loss_price = entry_price - atr * stop_loss_multiplier\n",
        "\n",
        "            outcome = 'Loss'\n",
        "            exit_price = stop_loss_price\n",
        "            exit_date = entry_date\n",
        "\n",
        "            future_data = df[(df['Ticker'] == ticker) & (df['Date'] > entry_date)].sort_values('Date')\n",
        "            if future_data.empty:\n",
        "                future_data = ticker_data[ticker_data['Date'] <= entry_date].iloc[-10:]\n",
        "\n",
        "            for _, row in future_data.iterrows():\n",
        "                if row['High'] >= target_price:\n",
        "                    outcome = 'Win'\n",
        "                    exit_price = target_price\n",
        "                    exit_date = row['Date']\n",
        "                    break\n",
        "                if row['Low'] <= stop_loss_price:\n",
        "                    outcome = 'Loss'\n",
        "                    exit_price = stop_loss_price\n",
        "                    exit_date = row['Date']\n",
        "                    break\n",
        "\n",
        "            profit = exit_price - entry_price\n",
        "            results.append({\n",
        "                'Ticker': ticker,\n",
        "                'Entry_Date': entry_date,\n",
        "                'Entry_Price': entry_price,\n",
        "                'Exit_Date': exit_date,\n",
        "                'Exit_Price': exit_price,\n",
        "                'Outcome': outcome,\n",
        "                'Profit': profit,\n",
        "                'ATR': atr,\n",
        "                'Target_Price': target_price,\n",
        "                'Stop_Loss': stop_loss_price\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        if results_df.empty:\n",
        "            logging.warning(\"Cell 7: No trades executed\")\n",
        "            return results_df\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"CREATE TABLE IF NOT EXISTS loss_tracker (Ticker TEXT, Loss_Date TEXT)\")\n",
        "        for _, row in results_df[results_df['Outcome'] == 'Loss'].iterrows():\n",
        "            cursor.execute(\"INSERT OR IGNORE INTO loss_tracker (Ticker, Loss_Date) VALUES (?, ?)\", (row['Ticker'], str(row['Exit_Date'])))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        win_rate = (results_df['Outcome'] == 'Win').mean() * 100\n",
        "        reward_risk = results_df[results_df['Outcome'] == 'Win']['Profit'].sum() / abs(results_df[results_df['Outcome'] == 'Loss']['Profit'].sum() + 1e-6)\n",
        "\n",
        "        results_df.to_csv(f\"{CONFIG['export_dir']}/backtest_results.csv\", index=False)\n",
        "        logging.info(f\"Cell 7: Backtest completed, {len(results_df)} trades, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7: Backtest completed, {len(results_df)} trades, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return results_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "backtest_results = backtest_strategy(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92zb3N6rxbe9",
        "outputId": "7d892de0-8f54-4e46-ae3f-d093089062e1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 7: No trades to backtest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Track Performance\n",
        "# Purpose: Track performance metrics from backtest results, including taxes.\n",
        "# Inputs: backtest_results.csv.\n",
        "# Outputs: performance_tracker.csv.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "\n",
        "def track_performance(backtest_results):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if backtest_results.empty:\n",
        "            logging.warning(\"Cell 8: No backtest results to track\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = backtest_results.copy()\n",
        "        df['Returns'] = df['Profit'] / df['Entry_Price']\n",
        "\n",
        "        net_returns = df['Returns'].copy()\n",
        "        net_returns[df['Outcome'] == 'Win'] *= 0.6  # 60% retained after taxes\n",
        "        net_profit = net_returns.sum()\n",
        "\n",
        "        win_rate = (df['Outcome'] == 'Win').mean() * 100\n",
        "        reward_risk = df[df['Outcome'] == 'Win']['Profit'].sum() * 0.6 / abs(df[df['Outcome'] == 'Loss']['Profit'].sum() + 1e-6)\n",
        "        sharpe_ratio = net_returns.mean() / (net_returns.std() + 1e-6) * np.sqrt(252)\n",
        "        cumulative_returns = (1 + net_returns).cumprod()\n",
        "        max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "\n",
        "        performance_df = df.copy()\n",
        "        performance_df['Win_Rate'] = win_rate\n",
        "        performance_df['Reward_Risk'] = reward_risk\n",
        "        performance_df['Sharpe'] = sharpe_ratio\n",
        "        performance_df['Max_Drawdown'] = max_drawdown\n",
        "        performance_df['Net_Profit'] = net_profit\n",
        "\n",
        "        performance_df.to_csv(f\"{CONFIG['export_dir']}/performance_tracker.csv\", index=False)\n",
        "        logging.info(f\"Cell 8: Performance tracked, Shape: {performance_df.shape}, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2f}%, Net Profit: {net_profit:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 8: Performance tracked, Shape: {performance_df.shape}, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2f}%, Net Profit: {net_profit:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return performance_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 8: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "file_path = f\"{CONFIG['export_dir']}/backtest_results.csv\"\n",
        "if os.path.exists(file_path):\n",
        "    backtest_results = pd.read_csv(file_path)\n",
        "else:\n",
        "    logging.warning(f\"Cell 8: backtest_results.csv not found, using empty DataFrame\")\n",
        "    backtest_results = pd.DataFrame()\n",
        "performance_df = track_performance(backtest_results)"
      ],
      "metadata": {
        "id": "NS8nsHMSyFq1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Retrain Model with Feedback\n",
        "# Purpose: Retrain model using backtest feedback as labels.\n",
        "# Inputs: processed_stock_data.csv, performance_tracker.csv.\n",
        "# Outputs: retrained_model.pkl, retrained_scaler.pkl.\n",
        "\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import logging\n",
        "import time\n",
        "import os\n",
        "\n",
        "def retrain_model(processed_data, performance_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if performance_df.empty or 'Win_Rate' not in performance_df.columns or performance_df['Win_Rate'].iloc[0] >= 50:\n",
        "            logging.info(\"Cell 9: Performance satisfactory or no data, no retrain needed\")\n",
        "            return None, None, None\n",
        "\n",
        "        logging.info(\"Cell 9: Win rate below 50%, triggering retrain\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/model_rf.pkl\", 'rb') as f:\n",
        "            rf_model = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/model_xgb.pkl\", 'rb') as f:\n",
        "            xgb_model = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/scaler.pkl\", 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/selected_features.pkl\", 'rb') as f:\n",
        "            selected_features = pickle.load(f)\n",
        "\n",
        "        df = processed_data.copy()\n",
        "        df['Target'] = (df.groupby('Ticker')['Close'].shift(-1) > df['Close']).astype(int)\n",
        "        df = df.dropna()\n",
        "        X = df[selected_features]\n",
        "        y = df['Target']\n",
        "\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        rf_model.fit(X_scaled, y)\n",
        "        xgb_model.fit(X_scaled, y)\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_model_rf.pkl\", 'wb') as f:\n",
        "            pickle.dump(rf_model, f)\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_model_xgb.pkl\", 'wb') as f:\n",
        "            pickle.dump(xgb_model, f)\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_scaler.pkl\", 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "\n",
        "        logging.info(f\"Cell 9: Models retrained, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 9: Models retrained, Time: {time.time() - start_time:.2f}s\")\n",
        "        return rf_model, xgb_model, scaler\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 9: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "file_path = f\"{CONFIG['export_dir']}/performance_tracker.csv\"\n",
        "if os.path.exists(file_path):\n",
        "    performance_df = pd.read_csv(file_path)\n",
        "else:\n",
        "    logging.warning(f\"Cell 9: performance_tracker.csv not found, using empty DataFrame\")\n",
        "    performance_df = pd.DataFrame()\n",
        "retrained_model_rf, retrained_model_xgb, retrained_scaler = retrain_model(processed_data, performance_df)"
      ],
      "metadata": {
        "id": "4_oiSKVAyazc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Fetch Intraday Data\n",
        "# Purpose: Fetch and cache intraday data for monitoring.\n",
        "# Inputs: CONFIG tickers, interval.\n",
        "# Outputs: intraday_data.csv.\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_intraday_data(tickers, interval=CONFIG['intraday_interval'], lookback_hours=CONFIG['intraday_lookback_hours']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/stock_data.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS intraday_data (\n",
        "            Date TEXT, Ticker TEXT, Open REAL, High REAL, Low REAL, Close REAL, Volume INTEGER,\n",
        "            PRIMARY KEY (Date, Ticker)\n",
        "        )''')\n",
        "\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=5)  # Sufficient for intraday lookback\n",
        "        cached_data = pd.read_sql_query(\n",
        "            f\"SELECT * FROM intraday_data WHERE Date >= ? AND Date <= ? AND Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "            conn, params=[start_date.strftime('%Y-%m-%d %H:%M:%S'), end_date.strftime('%Y-%m-%d %H:%M:%S')] + tickers\n",
        "        )\n",
        "        cached_data['Date'] = pd.to_datetime(cached_data['Date'])\n",
        "        cached_tickers = set(cached_data['Ticker'])\n",
        "        new_tickers = [t for t in tickers if t not in cached_tickers]\n",
        "\n",
        "        if len(cached_data) >= len(tickers) * lookback_hours * 0.7 and not new_tickers:\n",
        "            logging.info(f\"Cell 10: Loaded {len(cached_data)} intraday rows from cache\")\n",
        "            df = cached_data\n",
        "        else:\n",
        "            batch_size = 5\n",
        "            all_data = [cached_data] if not cached_data.empty else []\n",
        "            for i in range(0, len(new_tickers), batch_size):\n",
        "                batch_tickers = new_tickers[i:i + batch_size]\n",
        "                try:\n",
        "                    @retry(stop_max_attempt_number=3, wait_fixed=2)\n",
        "                    def fetch_yfinance(tickers, start, end, interval):\n",
        "                        return yf.download(tickers, start=start, end=end, interval=interval, progress=False, auto_adjust=True)\n",
        "\n",
        "                    data = fetch_yfinance(batch_tickers, start_date, end_date, interval)\n",
        "                    if data.empty:\n",
        "                        raise ValueError(f\"No intraday data for {batch_tickers}\")\n",
        "                    if isinstance(data.columns, pd.MultiIndex):\n",
        "                        data.columns = [f\"{col[0]}_{col[1]}\" for col in data.columns]\n",
        "                    data = data.reset_index().rename(columns={'Datetime': 'Date'})\n",
        "                    data['Date'] = pd.to_datetime(data['Date'])\n",
        "                    melted = data.melt(id_vars=['Date'], var_name='Metric', value_name='Value')\n",
        "                    melted['Ticker'] = melted['Metric'].str.split('_').str[-1]\n",
        "                    melted['Metric'] = melted['Metric'].str.split('_').str[0]\n",
        "                    df_batch = melted.pivot_table(index=['Date', 'Ticker'], columns='Metric', values='Value').reset_index()\n",
        "                    df_batch.columns.name = None\n",
        "                    df_batch.columns = [col.capitalize() for col in df_batch.columns]\n",
        "                    df_batch = df_batch.drop_duplicates(subset=['Date', 'Ticker'], keep='last')\n",
        "                    all_data.append(df_batch)\n",
        "                    logging.info(f\"Cell 10: Fetched intraday data for {batch_tickers}, Shape: {df_batch.shape}\")\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Cell 10: yfinance failed for {batch_tickers}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            df = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
        "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "            df = df.dropna(subset=['Date', 'Close', 'Ticker'])\n",
        "            df = df.sort_values(['Ticker', 'Date']).drop_duplicates(subset=['Date', 'Ticker'], keep='last').reset_index(drop=True)\n",
        "\n",
        "            if not df.empty:\n",
        "                existing_rows = pd.read_sql_query(\n",
        "                    f\"SELECT Date, Ticker FROM intraday_data WHERE Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "                    conn, params=tickers\n",
        "                )\n",
        "                existing_rows['Date'] = pd.to_datetime(existing_rows['Date'])\n",
        "                existing_set = set(existing_rows[['Date', 'Ticker']].itertuples(index=False, name=None))\n",
        "                new_rows = df[~df[['Date', 'Ticker']].apply(tuple, axis=1).isin(existing_set)]\n",
        "                if not new_rows.empty:\n",
        "                    new_rows.to_sql('intraday_data', conn, if_exists='append', index=False, method='multi')\n",
        "                    logging.info(f\"Cell 10: Cached {len(new_rows)} new intraday rows\")\n",
        "\n",
        "        conn.close()\n",
        "        df.to_csv(f\"{CONFIG['export_dir']}/intraday_data.csv\", index=False)\n",
        "        logging.info(f\"Cell 10: Intraday data fetched, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 10: Intraday data fetched, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 10: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "intraday_data = fetch_intraday_data(CONFIG['tickers'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYu52c5gylky",
        "outputId": "e210599a-0324-4c53-e72b-c84f9b995d3a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 10: Intraday data fetched, Shape: (9048, 7), Time: 6.47s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Diagnostics\n",
        "# Purpose: Run diagnostics on files, data, models, and sentiment.\n",
        "# Inputs: Various files in export_dir.\n",
        "# Outputs: Logs with diagnostic info.\n",
        "\n",
        "import os\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "def run_diagnostics():\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        files = os.listdir(CONFIG['export_dir'])\n",
        "        logging.info(f\"Cell 11: Files: {files}\")\n",
        "\n",
        "        processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "        nan_counts = processed_data.isna().sum()\n",
        "        logging.info(f\"Cell 11: Processed data shape: {processed_data.shape}, NaNs: {nan_counts[nan_counts > 0].to_dict()}\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/model_rf.pkl\", 'rb') as f:\n",
        "            pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/model_xgb.pkl\", 'rb') as f:\n",
        "            pickle.load(f)\n",
        "        logging.info(\"Cell 11: Models loaded successfully\")\n",
        "\n",
        "        sentiment_scores = {}\n",
        "        for ticker in CONFIG['tickers']:\n",
        "            sentiment_scores[ticker] = np.random.uniform(0, 1)  # Placeholder for diagnostics\n",
        "            logging.info(f\"Cell 11: Sentiment score for {ticker}: {sentiment_scores[ticker]:.2f}\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/sentiment_scores.json\", 'w') as f:\n",
        "            json.dump(sentiment_scores, f)\n",
        "        logging.info(f\"Cell 11: Sentiment scores saved: {sentiment_scores}\")\n",
        "        logging.info(f\"Cell 11: Diagnostics completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 11: Diagnostics completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 11: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "run_diagnostics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgLp2uYcyvUI",
        "outputId": "12f436b0-cd57-42b8-baae-a015bf7b55d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cell 11: Diagnostics completed, Time: 1.48s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Pipeline Orchestration\n",
        "# Purpose: Orchestrate the full pipeline execution.\n",
        "# Inputs: All previous outputs.\n",
        "# Outputs: Full pipeline run, updates files.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "def monitor_intraday(intraday_data, last_alert_time):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if intraday_data.empty:\n",
        "            logging.warning(\"Cell 12: Intraday data is empty, skipping monitoring\")\n",
        "            return last_alert_time\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_model.pkl\", 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_scaler.pkl\", 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/selected_features.pkl\", 'rb') as f:\n",
        "            selected_features = pickle.load(f)\n",
        "\n",
        "        processed_intraday = calculate_indicators(intraday_data.reset_index())\n",
        "        processed_intraday = processed_intraday.set_index('Date')\n",
        "\n",
        "        X = processed_intraday[selected_features].fillna(0)\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        confidences = model.predict_proba(X_scaled)[:, 1]\n",
        "        processed_intraday['Confidence'] = confidences\n",
        "\n",
        "        signals = processed_intraday[processed_intraday['Confidence'] >= CONFIG['confidence_threshold']]\n",
        "        signals = signals[['Ticker', 'Confidence']]\n",
        "\n",
        "        signals.to_csv(f\"{CONFIG['export_dir']}/intraday_signals.csv\", index=True)\n",
        "        logging.info(f\"Cell 12: Generated {len(signals)} intraday signals\")\n",
        "\n",
        "        return pd.to_datetime(datetime.now())\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 12: Monitor intraday failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run_pipeline():\n",
        "    start_time = time.time()\n",
        "    retries = 3\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Cell 3: Indicator Weighting (one-time, but call if db empty)\n",
        "            if not os.path.exists(f\"{CONFIG['export_dir']}/indicator_weights.db\"):\n",
        "                main()  # Call the main from Cell 3\n",
        "\n",
        "            # Cell 1: Fetch data\n",
        "            main()  # Call main from Cell 1\n",
        "\n",
        "            # Cell 2: Indicators\n",
        "            main()  # Call main from Cell 2\n",
        "\n",
        "            processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "            scaler, selected_features, predictions_df = train_model(processed_data)\n",
        "            trade_candidates_df = generate_trade_candidates(predictions_df)\n",
        "            backtest_results = backtest_strategy(processed_data, trade_candidates_df)\n",
        "            performance_df = track_performance(backtest_results)\n",
        "            retrained_model, retrained_scaler = retrain_model(processed_data, performance_df)\n",
        "            intraday_data = fetch_intraday_data(CONFIG['tickers'])\n",
        "            last_alert_time = None\n",
        "            if not intraday_data.empty:\n",
        "                last_alert_time = monitor_intraday(intraday_data, None)\n",
        "            logging.info(f\"Cell 12: Pipeline completed, Time: {time.time() - start_time:.2f}s\")\n",
        "            print(f\"Cell 12: Pipeline completed, Time: {time.time() - start_time:.2f}s\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Cell 12: Attempt {attempt+1} failed: {str(e)}\")\n",
        "            if attempt == retries - 1:\n",
        "                logging.error(f\"Cell 12: Pipeline failed after {retries} attempts: {str(e)}\")\n",
        "                print(f\"Cell 12: Pipeline failed: {str(e)}\")\n",
        "                raise\n",
        "            time.sleep(5)\n",
        "\n",
        "# Execute\n",
        "run_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fcs33U4Ty6go",
        "outputId": "20b88e0d-a07d-4083-84ff-75b67533303e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n",
            "Debug: Features available: ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
            "Debug: Data shape before training: (71731, 26)\n",
            "Debug: X_train shape: (71702, 5), y_train shape: (71702,)\n",
            "Debug: Target distribution: Target\n",
            "0    38463\n",
            "1    33239\n",
            "Name: count, dtype: int64\n",
            "Cell 4: Model trained, Avg CV Accuracy: 47.55%, Predictions shape: (29, 5), Time: 55.45s\n",
            "Cell 5: Trade candidates generated, Shape: (0, 6), Time: 0.01s\n",
            "Cell 7: No trades to backtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n",
            "Debug: Features available: ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
            "Debug: Data shape before training: (71731, 26)\n",
            "Debug: X_train shape: (71702, 5), y_train shape: (71702,)\n",
            "Debug: Target distribution: Target\n",
            "0    38463\n",
            "1    33239\n",
            "Name: count, dtype: int64\n",
            "Cell 4: Model trained, Avg CV Accuracy: 47.33%, Predictions shape: (29, 5), Time: 79.93s\n",
            "Cell 5: Trade candidates generated, Shape: (0, 6), Time: 0.02s\n",
            "Cell 7: No trades to backtest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
            "/tmp/ipython-input-4-53717724.py:26: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indicator weights calculated and stored in indicator_weights.db\n",
            "Debug: Features available: ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
            "Debug: Data shape before training: (71731, 26)\n",
            "Debug: X_train shape: (71702, 5), y_train shape: (71702,)\n",
            "Debug: Target distribution: Target\n",
            "0    38463\n",
            "1    33239\n",
            "Name: count, dtype: int64\n",
            "Cell 4: Model trained, Avg CV Accuracy: 47.39%, Predictions shape: (29, 5), Time: 67.04s\n",
            "Cell 5: Trade candidates generated, Shape: (0, 6), Time: 0.01s\n",
            "Cell 7: No trades to backtest\n",
            "Cell 12: Pipeline failed: too many values to unpack (expected 2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 2)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-1752122181.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Execute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-13-1752122181.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mbacktest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbacktest_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrade_candidates_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mperformance_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrack_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbacktest_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mretrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrained_scaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperformance_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mintraday_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_intraday_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tickers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mlast_alert_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Schedule Pipeline\n",
        "# Purpose: Schedule pipeline run during market hours.\n",
        "# Inputs: None\n",
        "# Outputs: Pipeline run if market open.\n",
        "\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "def schedule_pipeline():\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        current_time = pd.to_datetime(datetime.now()).tz_convert('US/Eastern')\n",
        "        market_open = current_time.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = current_time.replace(hour=16, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        if current_time.weekday() < 5 and market_open <= current_time <= market_close:\n",
        "            run_pipeline()\n",
        "        else:\n",
        "            logging.info(\"Cell 13: Market closed, skipping pipeline run\")\n",
        "\n",
        "        logging.info(f\"Cell 13: Schedule check completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 13: Schedule check completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 13: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "schedule_pipeline()"
      ],
      "metadata": {
        "id": "6p5Y8BIazDax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Modular Indicator Framework\n",
        "# Purpose: Load config for indicators, apply to data.\n",
        "# Inputs: processed_stock_data.csv, indicator_config.json.\n",
        "# Outputs: modular_processed_stock_data.csv.\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import time\n",
        "import pandas as pd\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def load_indicator_config(config_path=f\"{CONFIG['export_dir']}/indicator_config.json\"):\n",
        "    default_config = {\n",
        "        \"indicators\": {\n",
        "            \"RSI\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"Stoch_K\": {\"enabled\": True, \"params\": {\"k\": 14, \"smooth_k\": 3}},\n",
        "            \"MFI\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"MACD\": {\"enabled\": True, \"params\": {\"fast\": 12, \"slow\": 26, \"signal\": 9}},\n",
        "            \"ADX\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"Ichimoku_A\": {\"enabled\": True, \"params\": {\"tenkan\": 9, \"kijun\": 26, \"senkou\": 52}},\n",
        "            \"Supertrend\": {\"enabled\": True, \"params\": {\"length\": 10, \"multiplier\": 3}},\n",
        "            \"BB_Upper\": {\"enabled\": True, \"params\": {\"length\": 20, \"std\": 2}},\n",
        "            \"BB_Lower\": {\"enabled\": True, \"params\": {\"length\": 20, \"std\": 2}},\n",
        "            \"ATR\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"KC_Upper\": {\"enabled\": True, \"params\": {\"length\": 20, \"scalar\": 2, \"mamode\": \"ema\"}},\n",
        "            \"KC_Lower\": {\"enabled\": True, \"params\": {\"length\": 20, \"scalar\": 2, \"mamode\": \"ema\"}},\n",
        "            \"VWAP\": {\"enabled\": True, \"params\": {}},\n",
        "            \"AD\": {\"enabled\": True, \"params\": {}},\n",
        "            \"CMF\": {\"enabled\": True, \"params\": {\"length\": 20}},\n",
        "            \"OBV\": {\"enabled\": True, \"params\": {}},\n",
        "            \"Volume_Osc\": {\"enabled\": True, \"params\": {\"length\": 13}}\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        if os.path.exists(config_path):\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            logging.info(f\"Cell 14: Loaded indicator config from {config_path}\")\n",
        "        else:\n",
        "            with open(config_path, 'w') as f:\n",
        "                json.dump(default_config, f, indent=4)\n",
        "            config = default_config\n",
        "            logging.info(f\"Cell 14: Created default indicator config at {config_path}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 14: Failed to load/create config: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def apply_indicator_config(data):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        config = load_indicator_config()\n",
        "        selected_features = [name for name, params in config['indicators'].items() if params['enabled']]\n",
        "\n",
        "        def compute_indicators(group, ticker):\n",
        "            min_rows = 15\n",
        "            if len(group) < min_rows:\n",
        "                logging.warning(f\"Cell 14: Skipping {ticker}: insufficient data ({len(group)} rows)\")\n",
        "                return None\n",
        "\n",
        "            group = group.set_index('Date')\n",
        "            group = group[~group.index.duplicated(keep='last')]\n",
        "            group['Volume'] = np.log1p(group['Volume'].clip(lower=1))\n",
        "\n",
        "            for indicator, params in config['indicators'].items():\n",
        "                if not params['enabled']:\n",
        "                    continue\n",
        "                try:\n",
        "                    if indicator == 'RSI':\n",
        "                        group[indicator] = rsi(group['Close'], **params['params'])\n",
        "                    # Add similar for other indicators...\n",
        "                    # (To avoid length, assume the functions are defined in previous cells or add them here if needed)\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Cell 14: Failed to compute {indicator} for {ticker}: {str(e)}\")\n",
        "                    group[indicator] = np.nan\n",
        "\n",
        "            indicators = pd.DataFrame({\n",
        "                'Date': group.index,\n",
        "                'Ticker': np.full(len(group), ticker),\n",
        "                'Open': group['Open'],\n",
        "                'High': group['High'],\n",
        "                'Low': group['Low'],\n",
        "                'Close': group['Close'],\n",
        "                'Volume': group['Volume'],\n",
        "                **{col: group[col] for col in selected_features if col in group.columns}\n",
        "            })\n",
        "            return indicators\n",
        "\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.dropna(subset=['Date']).sort_values(['Ticker', 'Date'])\n",
        "        df = df.drop_duplicates(subset=['Date', 'Ticker'], keep='last')\n",
        "\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(compute_indicators)(df[df['Ticker'] == ticker], ticker)\n",
        "            for ticker in df['Ticker'].unique()\n",
        "        )\n",
        "        results = [r for r in results if r is not None]\n",
        "\n",
        "        if not results:\n",
        "            raise ValueError(\"Cell 14: No tickers had sufficient data for indicators\")\n",
        "\n",
        "        df = pd.concat(results, ignore_index=True)\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.dropna().drop_duplicates(subset=['Date', 'Ticker'], keep='last').reset_index(drop=True)\n",
        "\n",
        "        nan_counts = df.isna().sum()\n",
        "        if nan_counts.sum() > 0:\n",
        "            logging.warning(f\"Cell 14: NaN values in indicators: {nan_counts[nan_counts > 0].to_dict()}\")\n",
        "\n",
        "        df.to_csv(f\"{CONFIG['export_dir']}/modular_processed_stock_data.csv\", index=False)\n",
        "        logging.info(f\"Cell 14: Modular indicators calculated, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 14: Modular indicators calculated, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 14: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "apply_indicator_config(data)"
      ],
      "metadata": {
        "id": "inIBW9tPzLqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: CLI Interface\n",
        "# Purpose: Provide CLI for running the pipeline with custom tickers.\n",
        "# Inputs: Command line arguments.\n",
        "# Outputs: Pipeline run with specified tickers.\n",
        "\n",
        "import argparse\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Trading Pipeline\")\n",
        "    parser.add_argument('--tickers', nargs='+', default=CONFIG['tickers'], help=\"List of tickers to process\")\n",
        "    args = parser.parse_args()\n",
        "    CONFIG['tickers'] = args.tickers\n",
        "    run_pipeline()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "aZ8HGW2XzUke"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHBjbdO4aPHOB4tcMrGaOf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}