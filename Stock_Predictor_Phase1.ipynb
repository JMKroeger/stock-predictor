{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JMKroeger/stock-predictor/blob/main/Stock_Predictor_Phase1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 0: Setup Environment\n",
        "# Purpose: Initialize imports, configuration, logging, and test yfinance connectivity.\n",
        "# Inputs: None\n",
        "# Outputs: pipeline.log initialized, CONFIG defined, yfinance test result.\n",
        "\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from datetime import datetime\n",
        "from retrying import retry\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "try:\n",
        "    logging.basicConfig(\n",
        "        filename='pipeline.log',\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "    logging.info(\"Logging initialized\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up logging: {e}\")\n",
        "    raise\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'tickers': [\n",
        "        'AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'PLTR', 'AMD', 'AMZN', 'META', 'INTC',\n",
        "        'SPY', 'QQQ', 'NFLX', 'BA', 'JPM', 'V', 'PYPL', 'DIS', 'ADBE', 'CRM',\n",
        "        'CSCO', 'WMT', 'T', 'VZ', 'CMCSA', 'PFE', 'MRK', 'KO', 'PEP'\n",
        "    ],\n",
        "    'start_date': '2015-07-04',\n",
        "    'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'telegram_token': '7779970479:AAFJFop5XrTe7_dP1iGDoGVM-bdWNyYso8E',\n",
        "    'telegram_chat_id': '1591809098',\n",
        "    'export_dir': '.',\n",
        "    'confidence_threshold': 0.8,\n",
        "    'intraday_interval': '5m',\n",
        "    'intraday_lookback_hours': 4\n",
        "}\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "def test_yfinance():\n",
        "    \"\"\"Test yfinance with a single ticker and short date range.\"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        df = yf.download(\n",
        "            'AAPL',\n",
        "            start=(datetime.now() - pd.Timedelta(days=3)).strftime('%Y-%m-%d'),\n",
        "            end=datetime.now().strftime('%Y-%m-%d'),\n",
        "            progress=False\n",
        "        )\n",
        "        if df.empty:\n",
        "            logging.error(\"yfinance test failed: Empty DataFrame for AAPL\")\n",
        "            raise ValueError(\"Empty DataFrame\")\n",
        "        logging.info(\"yfinance test successful\")\n",
        "        print(\"yfinance test successful\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"yfinance test failed: {e}\")\n",
        "        print(f\"yfinance test failed: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"yfinance test took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"yfinance test took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main setup function.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 0: Environment setup\")\n",
        "    print(\"Setting up environment...\")\n",
        "\n",
        "    try:\n",
        "        # Test yfinance\n",
        "        test_yfinance()\n",
        "\n",
        "        # Log configuration\n",
        "        logging.info(f\"CONFIG: {len(CONFIG['tickers'])} tickers, date range {CONFIG['start_date']} to {CONFIG['end_date']}\")\n",
        "        print(\"Environment setup complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 0 failed: {e}\")\n",
        "        print(f\"Error in Cell 0: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 0 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Setup took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm80--IEMLmG",
        "outputId": "41d5e974-ab26-4092-cc61-96f056563682"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up environment...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-605753392.py:48: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "yfinance test successful\n",
            "yfinance test took 0.30 seconds\n",
            "Environment setup complete\n",
            "Setup took 0.30 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Environment Validation\n",
        "# Purpose: Validate Python version, library versions, and yfinance connectivity.\n",
        "# Inputs: CONFIG from Cell 0, pipeline.log\n",
        "# Outputs: Validation results logged to pipeline.log, printed to console\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "from retrying import retry\n",
        "import time\n",
        "\n",
        "# Ensure logging is configured (redundant if Cell 0 ran)\n",
        "try:\n",
        "    logging.basicConfig(\n",
        "        filename='pipeline.log',\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up logging: {e}\")\n",
        "    raise\n",
        "\n",
        "# Reuse CONFIG from Cell 0 (assumed global or passed)\n",
        "CONFIG = {\n",
        "    'tickers': [\n",
        "        'AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'PLTR', 'AMD', 'AMZN', 'META', 'INTC',\n",
        "        'SPY', 'QQQ', 'NFLX', 'BA', 'JPM', 'V', 'PYPL', 'DIS', 'ADBE', 'CRM',\n",
        "        'CSCO', 'WMT', 'T', 'VZ', 'CMCSA', 'PFE', 'MRK', 'KO', 'PEP'\n",
        "    ],\n",
        "    'start_date': '2015-07-04',\n",
        "    'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'telegram_token': '7779970479:AAFJFop5XrTe7_dP1iGDoGVM-bdWNyYso8E',\n",
        "    'telegram_chat_id': '1591809098',\n",
        "    'export_dir': '.'\n",
        "}\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "def test_yfinance():\n",
        "    \"\"\"Test yfinance with a single ticker and short date range.\"\"\"\n",
        "    try:\n",
        "        df = yf.download(\n",
        "            'AAPL',\n",
        "            start=(datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'),\n",
        "            end=datetime.now().strftime('%Y-%m-%d'),\n",
        "            progress=False\n",
        "        )\n",
        "        if df.empty:\n",
        "            logging.error(\"yfinance validation failed: Empty DataFrame for AAPL\")\n",
        "            raise ValueError(\"Empty DataFrame\")\n",
        "        logging.info(\"yfinance validation successful\")\n",
        "        print(\"yfinance validation successful\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logging.error(f\"yfinance validation failed: {e}\")\n",
        "        print(f\"yfinance validation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "def validate_environment():\n",
        "    \"\"\"Validate Python and library versions.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 1: Environment validation\")\n",
        "    print(\"Validating environment...\")\n",
        "\n",
        "    try:\n",
        "        # Check Python version\n",
        "        if sys.version_info[:2] != (3, 11):\n",
        "            logging.error(\"Python 3.11 required\")\n",
        "            raise ValueError(\"Python 3.11 required\")\n",
        "        logging.info(\"Python version validated: 3.11\")\n",
        "        print(f\"Python: {sys.version.split()[0]}\")\n",
        "\n",
        "        # Check library versions\n",
        "        libraries = {\n",
        "            'pandas': pd.__version__,\n",
        "            'numpy': np.__version__,\n",
        "            'yfinance': yf.__version__\n",
        "        }\n",
        "        for lib, version in libraries.items():\n",
        "            logging.info(f\"{lib}: {version}\")\n",
        "            print(f\"{lib}: {version}\")\n",
        "\n",
        "        # Test yfinance\n",
        "        test_yfinance()\n",
        "\n",
        "        logging.info(\"Cell 1: Validation successful\")\n",
        "        print(\"Environment validation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 1: Validation failed: {e}\")\n",
        "        print(f\"Error in Cell 1: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 1 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Validation took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    validate_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgu22brMMNdI",
        "outputId": "fac01923-18c1-4941-87b2-1c749121299b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating environment...\n",
            "Python: 3.11.13\n",
            "pandas: 2.3.1\n",
            "numpy: 2.0.2\n",
            "yfinance: 0.2.65\n",
            "yfinance validation successful\n",
            "Environment validation complete\n",
            "Validation took 0.19 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2311978090.py:44: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Fetch and Transform Data\n",
        "# Purpose: Fetch historical stock data using yfinance, cache to SQLite, save to raw_stock_data.csv.\n",
        "# Inputs: CONFIG from Cell 0, yfinance API.\n",
        "# Outputs: stock_data.db, raw_stock_data.csv, logs to pipeline.log.\n",
        "\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import sqlite3\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from retrying import retry\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "# Reuse CONFIG\n",
        "CONFIG = {\n",
        "    'tickers': [\n",
        "        'AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'PLTR', 'AMD', 'AMZN', 'META', 'INTC',\n",
        "        'SPY', 'QQQ', 'NFLX', 'BA', 'JPM', 'V', 'PYPL', 'DIS', 'ADBE', 'CRM',\n",
        "        'CSCO', 'WMT', 'T', 'VZ', 'CMCSA', 'PFE', 'MRK', 'KO', 'PEP'\n",
        "    ],\n",
        "    'start_date': '2015-07-04',\n",
        "    'end_date': datetime.now().strftime('%Y-%m-%d'),\n",
        "    'telegram_token': '7779970479:AAFJFop5XrTe7_dP1iGDoGVM-bdWNyYso8E',\n",
        "    'telegram_chat_id': '1591809098'\n",
        "}\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "def fetch_historical_data(ticker):\n",
        "    \"\"\"Fetch historical data with retry logic.\"\"\"\n",
        "    try:\n",
        "        # Fetch data with single ticker to avoid multi-index\n",
        "        df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
        "        if df.empty:\n",
        "            logging.warning(f\"No data fetched for {ticker}\")\n",
        "            return None\n",
        "        # Handle multi-index or unexpected column names\n",
        "        if isinstance(df.columns, pd.MultiIndex):\n",
        "            df.columns = df.columns.get_level_values(0)\n",
        "        df['Ticker'] = ticker\n",
        "        df['Date'] = df.index\n",
        "        df = df.reset_index(drop=True)\n",
        "        # Standardize column names\n",
        "        col_map = {\n",
        "            'Open': 'Open', 'High': 'High', 'Low': 'Low', 'Close': 'Close', 'Volume': 'Volume',\n",
        "            'Adj Close': 'Adj_Close', 'Adjusted Close': 'Adj_Close'\n",
        "        }\n",
        "        df = df.rename(columns={k: v for k, v in col_map.items() if k in df.columns})\n",
        "        required_cols = ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj_Close']\n",
        "        # Ensure all required columns exist\n",
        "        for col in required_cols:\n",
        "            if col not in df.columns:\n",
        "                df[col] = df['Close'] if col == 'Adj_Close' else None\n",
        "        return df[required_cols]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def save_to_sqlite(df, conn):\n",
        "    \"\"\"Save DataFrame to SQLite with consistent schema.\"\"\"\n",
        "    try:\n",
        "        df.to_sql('historical_data', conn, if_exists='append', index=False)\n",
        "        logging.debug(f\"Data saved to SQLite for {df['Ticker'].iloc[0]}\")  # Changed to debug\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving to SQLite: {e}\")\n",
        "        raise\n",
        "\n",
        "def initialize_sqlite():\n",
        "    \"\"\"Initialize SQLite database with historical_data table.\"\"\"\n",
        "    try:\n",
        "        conn = sqlite3.connect('stock_data.db')\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"DROP TABLE IF EXISTS historical_data\")\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS historical_data (\n",
        "                Date TEXT,\n",
        "                Ticker TEXT,\n",
        "                Open REAL,\n",
        "                High REAL,\n",
        "                Low REAL,\n",
        "                Close REAL,\n",
        "                Volume INTEGER,\n",
        "                Adj_Close REAL\n",
        "            )\n",
        "        \"\"\")\n",
        "        conn.commit()\n",
        "        return conn\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing SQLite: {e}\")\n",
        "        raise\n",
        "\n",
        "def main():\n",
        "    \"\"\"Fetch and store historical data.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 2: Data fetching\")\n",
        "    print(\"Fetching historical data...\")\n",
        "\n",
        "    try:\n",
        "        # Initialize SQLite database\n",
        "        conn = initialize_sqlite()\n",
        "\n",
        "        # Fetch data for each ticker\n",
        "        all_data = []\n",
        "        for ticker in CONFIG['tickers']:\n",
        "            df = fetch_historical_data(ticker)\n",
        "            if df is not None:\n",
        "                all_data.append(df)\n",
        "                save_to_sqlite(df, conn)\n",
        "                logging.debug(f\"Fetched and saved data for {ticker}\")  # Changed to debug\n",
        "\n",
        "        # Close SQLite connection\n",
        "        conn.close()\n",
        "\n",
        "        # Combine and save to CSV\n",
        "        if all_data:\n",
        "            combined_df = pd.concat(all_data, ignore_index=True)\n",
        "            combined_df.to_csv('raw_stock_data.csv', index=False)\n",
        "            logging.info(\"Saved data to raw_stock_data.csv\")\n",
        "            print(\"Data saved to raw_stock_data.csv\")\n",
        "        else:\n",
        "            logging.warning(\"No data fetched for any ticker\")\n",
        "            print(\"No data fetched\")\n",
        "\n",
        "        logging.info(\"Cell 2: Data fetching successful\")\n",
        "        print(\"Data fetching complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 2: Failed: {e}\")\n",
        "        print(f\"Error in Cell 2: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 2 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Data fetching took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN4AQZ__MNaU",
        "outputId": "38c66644-a475-4738-e0f0-abd9d600181a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching historical data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n",
            "/tmp/ipython-input-5-2271266795.py:39: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  df = yf.download(ticker, start=CONFIG['start_date'], end=CONFIG['end_date'], progress=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to raw_stock_data.csv\n",
            "Data fetching complete\n",
            "Data fetching took 13.18 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Calculate Technical Indicators\n",
        "# Purpose: Compute technical indicators for historical data to enhance ML predictions.\n",
        "# Inputs: raw_stock_data.csv, stock_data.db.\n",
        "# Outputs: processed_stock_data.csv, logs to pipeline.log.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas_ta as ta\n",
        "import logging\n",
        "import sqlite3\n",
        "import time\n",
        "import os\n",
        "import yfinance as yf\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    filename='pipeline.log',\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    \"\"\"Calculate technical indicators for a single ticker using pandas_ta.\"\"\"\n",
        "    try:\n",
        "        # Momentum\n",
        "        df['RSI'] = ta.rsi(df['Close'], length=14)\n",
        "        stoch = ta.stoch(df['High'], df['Low'], df['Close'], k=14, d=3, smooth_k=3)\n",
        "        df['Stoch_K'] = stoch['STOCHk_14_3_3']\n",
        "        df['MFI'] = ta.mfi(df['High'], df['Low'], df['Close'], df['Volume'], length=14)\n",
        "\n",
        "        # Trend\n",
        "        macd = ta.macd(df['Close'], fast=12, slow=26, signal=9)\n",
        "        df['MACD'] = macd['MACD_12_26_9']\n",
        "        adx = ta.adx(df['High'], df['Low'], df['Close'], length=14)\n",
        "        df['ADX'] = adx['ADX_14']\n",
        "        ich = ta.ichimoku(df['High'], df['Low'], df['Close'], tenkan=9, kijun=26, senkou=52)\n",
        "        df['Ichimoku_A'] = ich[0]['ISA_9']\n",
        "        supertrend = ta.supertrend(df['High'], df['Low'], df['Close'], length=10, multiplier=3)\n",
        "        df['Supertrend'] = supertrend['SUPERT_10_3.0']\n",
        "\n",
        "        # Volatility\n",
        "        bb = ta.bbands(df['Close'], length=20, std=2)\n",
        "        df['BB_Upper'] = bb['BBU_20_2.0']\n",
        "        df['BB_Lower'] = bb['BBL_20_2.0']\n",
        "        df['ATR'] = ta.atr(df['High'], df['Low'], df['Close'], length=14)\n",
        "        kc = ta.kc(df['High'], df['Low'], df['Close'], length=20, scalar=2, mamode=\"ema\", atr_length=10)\n",
        "        df['KC_Upper'] = kc['KCUe_20_2.0']\n",
        "        df['KC_Lower'] = kc['KCLe_20_2.0']\n",
        "\n",
        "        # Volume\n",
        "        df['VWAP'] = ta.vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        df['AD'] = ta.ad(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        df['CMF'] = ta.cmf(df['High'], df['Low'], df['Close'], df['Volume'], length=20)\n",
        "        df['OBV'] = ta.obv(df['Close'], df['Volume'])\n",
        "        df['Volume_Osc'] = ta.fi(df['Close'], df['Volume'], length=13)  # Force Index as proxy for Volume Oscillator\n",
        "\n",
        "        return df.fillna(0)  # Fill any remaining NaNs with 0 for ML compatibility\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating indicators: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_sentiment_score(ticker):\n",
        "    \"\"\"Fetch recent news from yfinance and compute average sentiment using VADER.\"\"\"\n",
        "    try:\n",
        "        t = yf.Ticker(ticker)\n",
        "        news = t.news\n",
        "        if not news:\n",
        "            logging.debug(f\"No news for {ticker}, using neutral sentiment\")  # Changed to debug\n",
        "            return 0.5\n",
        "        analyzer = SentimentIntensityAnalyzer()\n",
        "        scores = []\n",
        "        for article in news:\n",
        "            title = article.get('title')\n",
        "            summary = article.get('summary', '')\n",
        "            text = title if title else summary\n",
        "            if text and isinstance(text, str):\n",
        "                scores.append(analyzer.polarity_scores(text)['compound'])\n",
        "        if not scores:\n",
        "            logging.debug(f\"No valid text for sentiment in news for {ticker}\")  # Changed to debug\n",
        "            return 0.5\n",
        "        return np.mean(scores)\n",
        "    except Exception as e:\n",
        "        logging.warning(f\"Sentiment analysis failed for {ticker}: {e}\")\n",
        "        return 0.5\n",
        "\n",
        "def main():\n",
        "    \"\"\"Calculate indicators and save results.\"\"\"\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Starting Cell 3: Indicator calculation\")\n",
        "    print(\"Calculating indicators...\")\n",
        "\n",
        "    try:\n",
        "        # Check if input file exists\n",
        "        if not os.path.exists('raw_stock_data.csv'):\n",
        "            logging.error(\"Input file raw_stock_data.csv not found\")\n",
        "            raise FileNotFoundError(\"raw_stock_data.csv not found\")\n",
        "\n",
        "        # Load data from CSV\n",
        "        df = pd.read_csv('raw_stock_data.csv')\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Compute sentiment scores\n",
        "        sentiment_dict = {ticker: get_sentiment_score(ticker) for ticker in df['Ticker'].unique()}\n",
        "        df['Sentiment'] = df['Ticker'].map(sentiment_dict)\n",
        "\n",
        "        # Initialize SQLite connection\n",
        "        conn = sqlite3.connect('stock_data.db')\n",
        "\n",
        "        # Calculate indicators for each ticker\n",
        "        all_data = []\n",
        "        for ticker in df['Ticker'].unique():\n",
        "            ticker_df = df[df['Ticker'] == ticker].sort_values('Date').set_index('Date')\n",
        "            ticker_df = calculate_indicators(ticker_df)\n",
        "            if ticker_df is not None:\n",
        "                ticker_df['Ticker'] = ticker\n",
        "                all_data.append(ticker_df.reset_index())\n",
        "\n",
        "        # Combine and save\n",
        "        if all_data:\n",
        "            processed_df = pd.concat(all_data, ignore_index=True)\n",
        "            processed_df.to_csv('processed_stock_data.csv', index=False)\n",
        "            processed_df.to_sql('processed_data', conn, if_exists='replace', index=False)\n",
        "            logging.info(\"Saved processed data to processed_stock_data.csv and SQLite\")\n",
        "            print(\"Processed data saved\")\n",
        "        else:\n",
        "            logging.warning(\"No processed data generated\")\n",
        "            print(\"No processed data\")\n",
        "\n",
        "        conn.close()\n",
        "        logging.info(\"Cell 3: Indicator calculation successful\")\n",
        "        print(\"Indicator calculation complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 3: Failed: {e}\")\n",
        "        print(f\"Error in Cell 3: {e}\")\n",
        "        raise\n",
        "    finally:\n",
        "        elapsed_time = time.time() - start_time\n",
        "        logging.info(f\"Cell 3 took {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Indicator calculation took {elapsed_time:.2f} seconds\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "ryewBNToMNVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "2b59019d-9676-4ec8-e461-81425926bd64"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "DistributionNotFound",
          "evalue": "The 'pandas_ta' distribution was not found and is required by the application",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDistributionNotFound\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-2648878108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas_ta\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas_ta/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0m_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pandas_ta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Normalize case for Windows systems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_provider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistribution\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expected str, Requirement, or Distribution\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mget_provider\u001b[0;34m(moduleOrReq)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;34m\"\"\"Return an IResourceProvider for the named module or requirement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRequirement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mrequire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmoduleOrReq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mrequire\u001b[0;34m(self, *requirements)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mincluded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meven\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mwere\u001b[0m \u001b[0malready\u001b[0m \u001b[0mactivated\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mworking\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \"\"\"\n\u001b[0;32m-> 1070\u001b[0;31m         \u001b[0mneeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirements\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mneeded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36mresolve\u001b[0;34m(self, requirements, env, installer, replace_conflicting, extras)\u001b[0m\n\u001b[1;32m    895\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m             dist = self._resolve_dist(\n\u001b[0m\u001b[1;32m    898\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace_conflicting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstaller\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_activate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m_resolve_dist\u001b[0;34m(self, req, best, replace_conflicting, env, installer, required_by, to_activate)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m                     \u001b[0mrequirers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequired_by\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mDistributionNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m             \u001b[0mto_activate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDistributionNotFound\u001b[0m: The 'pandas_ta' distribution was not found and is required by the application"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3Z: Initialization for Indicator Weighting\n",
        "def initialize_indicator_weights(tickers, start_date='2015-07-04', end_date=CONFIG['end_date']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        # Check if weights already exist\n",
        "        if os.path.exists(f\"{CONFIG['export_dir']}/indicator_weights.db\"):\n",
        "            conn = sqlite3.connect(f\"{CONFIG['export_dir']}/indicator_weights.db\")\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' AND name='indicator_weights'\")\n",
        "            if cursor.fetchone():\n",
        "                logging.info(\"Cell 16: Indicator weights already exist, skipping initialization\")\n",
        "                conn.close()\n",
        "                return\n",
        "\n",
        "        # Fetch historical data\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/stock_data.db\")\n",
        "        cached_data = pd.read_sql_query(\n",
        "            f\"SELECT * FROM raw_stock_data WHERE Date >= ? AND Date <= ? AND Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "            conn, params=[start_date, end_date] + tickers\n",
        "        )\n",
        "        cached_data['Date'] = pd.to_datetime(cached_data['Date'], utc=True)\n",
        "        conn.close()\n",
        "\n",
        "        if cached_data.empty:\n",
        "            raise ValueError(\"Cell 16: No historical data available\")\n",
        "\n",
        "        # Calculate indicators\n",
        "        df = calculate_technicals(cached_data)\n",
        "        features = [col for col in df.columns if col not in ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume']]\n",
        "\n",
        "        # Define target (price increase > 1%)\n",
        "        df['Next_Close'] = df.groupby('Ticker')['Close'].shift(-1)\n",
        "        df['Target'] = (df['Next_Close'] > df['Close'] * 1.01).astype(int).fillna(0)\n",
        "        df = df.dropna(subset=['Target'] + features)\n",
        "\n",
        "        X = df[features].fillna(0)\n",
        "        y = df['Target']\n",
        "\n",
        "        # Train Random Forest for feature importance\n",
        "        rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
        "        rf.fit(X, y)\n",
        "\n",
        "        # Compute weights for individual indicators and combinations\n",
        "        importance_df = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': rf.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "\n",
        "        # Test key combinations\n",
        "        combo_weights = []\n",
        "        combos = [\n",
        "            ['Golden_Cross', 'RSI'],\n",
        "            ['Stochastic_K', 'ADX'],\n",
        "            ['RSI_MACD_Crossover', 'BB_Stochastic'],\n",
        "            ['MACD', 'BB_Upper']\n",
        "        ]\n",
        "        for combo in combos:\n",
        "            if all(f in features for f in combo):\n",
        "                combo_score = importance_df[importance_df['Feature'].isin(combo)]['Importance'].sum()\n",
        "                combo_weights.append({'Combo': '+'.join(combo), 'Weight': combo_score})\n",
        "\n",
        "        # Store weights in database\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/indicator_weights.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS indicator_weights (\n",
        "                Feature TEXT, Weight REAL, Type TEXT, PRIMARY KEY (Feature)\n",
        "            )\n",
        "        ''')\n",
        "        for _, row in importance_df.iterrows():\n",
        "            cursor.execute(\"INSERT OR REPLACE INTO indicator_weights (Feature, Weight, Type) VALUES (?, ?, ?)\",\n",
        "                          (row['Feature'], row['Importance'], 'Individual'))\n",
        "        for combo in combo_weights:\n",
        "            cursor.execute(\"INSERT OR REPLACE INTO indicator_weights (Feature, Weight, Type) VALUES (?, ?, ?)\",\n",
        "                          (combo['Combo'], combo['Weight'], 'Combination'))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        importance_df.to_csv(f\"{CONFIG['export_dir']}/indicator_weights.csv\", index=False)\n",
        "        logging.info(f\"Cell 16: Indicator weights initialized, Features: {len(features)}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 16: Indicator weights initialized, Features: {len(features)}, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 16: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "initialize_indicator_weights(CONFIG['tickers'])"
      ],
      "metadata": {
        "id": "7F_nbJxrlUS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: Model Training and Feature Selection\n",
        "def train_model(data):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df = df.dropna(subset=['Close', 'Ticker'])\n",
        "\n",
        "        sentiment_path = f\"{CONFIG['export_dir']}/sentiment_scores.json\"\n",
        "        if os.path.exists(sentiment_path):\n",
        "            with open(sentiment_path, 'r') as f:\n",
        "                sentiment_scores = json.load(f)\n",
        "            df['Sentiment'] = df['Ticker'].map(sentiment_scores).fillna(0.5)\n",
        "        else:\n",
        "            logging.warning(\"Cell 4: No sentiment scores found, using neutral values\")\n",
        "            df['Sentiment'] = 0.5\n",
        "\n",
        "        features = [\n",
        "            'RSI_lag1', 'Volatility_5', 'AD', 'CMF', 'ADX', 'MACD', 'Signal',\n",
        "            'MACD_Histogram', 'RSI', 'OBV', 'ATR', 'Volume_MA5', 'RSI_M',\n",
        "            'Stoch_RSI', 'Stochastic_K', 'Volume_Oscillator', 'Sentiment'\n",
        "        ]\n",
        "\n",
        "        try:\n",
        "            conn = sqlite3.connect(f\"{CONFIG['export_dir']}/indicator_weights.db\")\n",
        "            weights_df = pd.read_sql_query(\"SELECT Feature, Weight FROM indicator_weights WHERE Type='Individual'\", conn)\n",
        "            conn.close()\n",
        "            weights = dict(zip(weights_df['Feature'], weights_df['Weight']))\n",
        "            logging.info(f\"Cell 4: Loaded weights: {weights}\")\n",
        "            weighted_features = sorted(\n",
        "                [f for f in features if f in weights],\n",
        "                key=lambda x: weights.get(x, 0),\n",
        "                reverse=True\n",
        "            )[:10]\n",
        "            selected_features = [col for col in df.columns if col in weighted_features]\n",
        "            logging.info(f\"Cell 4: Using {len(selected_features)} weighted features: {selected_features}\")\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Cell 4: Failed to load indicator weights: {str(e)}, using all features\")\n",
        "            selected_features = [col for col in df.columns if col in features]\n",
        "\n",
        "        for file in ['model_rf.pkl', 'model_xgb.pkl', 'scaler.pkl', 'selected_features.pkl']:\n",
        "            file_path = f\"{CONFIG['export_dir']}/{file}\"\n",
        "            if os.path.exists(file_path):\n",
        "                os.remove(file_path)\n",
        "                logging.info(f\"Cell 4: Cleared {file_path}\")\n",
        "\n",
        "        if not selected_features:\n",
        "            selected_features = [col for col in df.columns if col in features]\n",
        "            logging.warning(f\"Cell 4: No weighted features, using all: {selected_features}\")\n",
        "\n",
        "        missing_features = [f for f in features if f not in df.columns]\n",
        "        if missing_features:\n",
        "            logging.warning(f\"Cell 4: Missing features: {missing_features}, filling with zeros\")\n",
        "            for f in missing_features:\n",
        "                df[f] = 0\n",
        "\n",
        "        df['Volume'] = np.log1p(df['Volume'].clip(lower=1))\n",
        "        df['VWAP'] = df['VWAP'].clip(lower=0, upper=df['Close'].max() * 2) if 'VWAP' in df.columns else df['Close']\n",
        "        df['Volatility_5'] = df['Volatility_5'].clip(lower=0, upper=0.1)\n",
        "        df['Volume_Spike'] = df['Volume_Spike'].clip(lower=0, upper=1) if 'Volume_Spike' in df.columns else 0\n",
        "\n",
        "        logging.info(f\"Cell 4: Features available: {selected_features}\")\n",
        "        print(f\"Debug: Features available: {selected_features}\")\n",
        "        print(f\"Debug: Data shape before training: {df.shape}\")\n",
        "\n",
        "        volatility = df.groupby('Ticker')['Close'].pct_change().rolling(window=5).std().groupby(df['Ticker']).last()\n",
        "        df['Next_Close'] = df.groupby('Ticker')['Close'].shift(-1)\n",
        "        df['Target'] = df.apply(\n",
        "            lambda row: 1 if pd.notna(row['Next_Close']) and row['Next_Close'] > row['Close'] * (1 + volatility.get(row['Ticker'], 0.003) * 0.15) else 0,\n",
        "            axis=1\n",
        "        )\n",
        "        df = df.drop(columns=['Next_Close']).dropna(subset=['Target', 'Close'] + selected_features)\n",
        "\n",
        "        latest_date = df['Date'].max()\n",
        "        train_data = df[df['Date'] < latest_date]\n",
        "        pred_data = df[df['Date'] == latest_date]\n",
        "\n",
        "        if train_data.empty or pred_data.empty:\n",
        "            raise ValueError(\"Cell 4: Insufficient data for training or prediction\")\n",
        "\n",
        "        X_train = train_data[selected_features]\n",
        "        y_train = train_data['Target']\n",
        "        X_train = X_train.fillna(0)\n",
        "        y_train = y_train.fillna(0).astype(int)\n",
        "\n",
        "        logging.info(f\"Cell 4: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        logging.info(f\"Cell 4: Target distribution: {y_train.value_counts().to_dict()}\")\n",
        "        print(f\"Debug: X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "        print(f\"Debug: Target distribution: {y_train.value_counts()}\")\n",
        "\n",
        "        def train_split(train_idx, test_idx, X_train, y_train, selected_features):\n",
        "            X_train_split, X_test_split = X_train.iloc[train_idx], X_train.iloc[test_idx]\n",
        "            y_train_split, y_test_split = y_train.iloc[train_idx], y_train.iloc[test_idx]\n",
        "            scaler = StandardScaler()\n",
        "            X_train_scaled = scaler.fit_transform(X_train_split)\n",
        "            scaler.feature_names_in_ = selected_features\n",
        "            X_test_scaled = scaler.transform(X_test_split)\n",
        "\n",
        "            def rf_objective(trial):\n",
        "                rf_params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
        "                    'min_samples_split': trial.suggest_int('min_samples_split', 5, 10),\n",
        "                    'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 5)\n",
        "                }\n",
        "                rf_model = RandomForestClassifier(**rf_params, random_state=42, class_weight='balanced')\n",
        "                rf_model.fit(X_train_scaled, y_train_split)\n",
        "                rf_pred = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "                return precision_score(y_test_split, (rf_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            def xgb_objective(trial):\n",
        "                xgb_params = {\n",
        "                    'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "                    'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
        "                    'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05)\n",
        "                }\n",
        "                xgb_model = XGBClassifier(**xgb_params, random_state=42, scale_pos_weight=(y_train_split==0).sum()/(y_train_split==1).sum() + 1e-6)\n",
        "                xgb_model.fit(X_train_scaled, y_train_split)\n",
        "                xgb_pred = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "                return precision_score(y_test_split, (xgb_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            rf_study = optuna.create_study(direction='maximize')\n",
        "            rf_study.optimize(rf_objective, n_trials=5)\n",
        "            rf_model = RandomForestClassifier(**rf_study.best_params, random_state=42, class_weight='balanced')\n",
        "            rf_model.fit(X_train_scaled, y_train_split)\n",
        "            rf_pred = rf_model.predict_proba(X_test_scaled)[:, 1]\n",
        "            rf_score = precision_score(y_test_split, (rf_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            xgb_study = optuna.create_study(direction='maximize')\n",
        "            xgb_study.optimize(xgb_objective, n_trials=5)\n",
        "            xgb_model = XGBClassifier(**xgb_study.best_params, random_state=42, scale_pos_weight=(y_train_split==0).sum()/(y_train_split==1).sum() + 1e-6)\n",
        "            xgb_model.fit(X_train_scaled, y_train_split)\n",
        "            xgb_pred = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
        "            xgb_score = precision_score(y_test_split, (xgb_pred > 0.5).astype(int), zero_division=0)\n",
        "\n",
        "            return rf_score, xgb_score, rf_model, xgb_model, scaler\n",
        "\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(train_split)(train_idx, test_idx, X_train, y_train, selected_features)\n",
        "            for train_idx, test_idx in tscv.split(X_train)\n",
        "        )\n",
        "        rf_scores = [r[0] for r in results]\n",
        "        xgb_scores = [r[1] for r in results]\n",
        "        rf_model = results[-1][2]\n",
        "        xgb_model = results[-1][3]\n",
        "        scaler = results[-1][4]\n",
        "\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_pred_scaled = scaler.transform(pred_data[selected_features].fillna(0))\n",
        "\n",
        "        rf_model.fit(X_train_scaled, y_train)\n",
        "        xgb_model.fit(X_train_scaled, y_train)\n",
        "        rf_pred = rf_model.predict_proba(X_pred_scaled)[:, 1]\n",
        "        xgb_pred = xgb_model.predict_proba(X_pred_scaled)[:, 1]\n",
        "        confidence = (rf_pred + xgb_pred) / 2\n",
        "\n",
        "        predictions_df = pred_data[['Ticker', 'Date', 'Close']].copy()\n",
        "        predictions_df['Confidence'] = confidence\n",
        "        predictions_df['Prediction'] = (confidence > 0.5).astype(int)\n",
        "        predictions_df = predictions_df.sort_values('Confidence', ascending=False)\n",
        "\n",
        "        rf_importance = pd.DataFrame({\n",
        "            'Feature': selected_features,\n",
        "            'Importance': rf_model.feature_importances_\n",
        "        }).sort_values('Importance', ascending=False)\n",
        "        logging.info(f\"Cell 4: RF Feature Importance (Top 5): {rf_importance.head().to_dict()}\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/model_rf.pkl\", 'wb') as f:\n",
        "            pickle.dump(rf_model, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/model_rf.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/model_xgb.pkl\", 'wb') as f:\n",
        "            pickle.dump(xgb_model, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/model_xgb.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/scaler.pkl\", 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/scaler.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/selected_features.pkl\", 'wb') as f:\n",
        "            pickle.dump(selected_features, f)\n",
        "        logging.info(f\"Cell 4: Saved {CONFIG['export_dir']}/selected_features.pkl\")\n",
        "        predictions_df.to_csv(f\"{CONFIG['export_dir']}/predictions.csv\", index=False)\n",
        "\n",
        "        avg_cv_accuracy = (np.mean(rf_scores) + np.mean(xgb_scores)) / 2 * 100\n",
        "        logging.info(f\"Cell 4: Model trained, Avg CV Accuracy: {avg_cv_accuracy:.2f}%, Predictions shape: {predictions_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 4: Model trained, Avg CV Accuracy: {avg_cv_accuracy:.2f}%, Predictions shape: {predictions_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return scaler, selected_features, predictions_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 4: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "scaler, selected_features, predictions_df = train_model(processed_data)"
      ],
      "metadata": {
        "id": "u5YLWI08MNQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Generate Trade Candidates\n",
        "def generate_trade_candidates(predictions_df, confidence_threshold=CONFIG['confidence_threshold']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if predictions_df.empty:\n",
        "            raise ValueError(\"Cell 5: Empty predictions dataframe\")\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        loss_data = pd.read_sql(\"SELECT * FROM loss_tracker\", conn)\n",
        "        conn.close()\n",
        "\n",
        "        current_date = datetime.now()\n",
        "        excluded_tickers = []\n",
        "        for _, row in loss_data.iterrows():\n",
        "            loss_date = pd.to_datetime(row['Loss_Date'])\n",
        "            if (current_date - loss_date).days < 35:\n",
        "                excluded_tickers.append(row['Ticker'])\n",
        "        excluded_tickers = list(set(excluded_tickers))\n",
        "\n",
        "        df = predictions_df.copy()\n",
        "        df = df[~df['Ticker'].isin(excluded_tickers)]\n",
        "        volatility = df.groupby('Ticker')['Close'].pct_change().rolling(window=5).std().groupby(df['Ticker']).last()\n",
        "        df['Volatility'] = df['Ticker'].map(volatility).fillna(1.0)\n",
        "\n",
        "        candidates = df[df['Confidence'] >= confidence_threshold][['Ticker', 'Date', 'Close', 'Confidence', 'Prediction', 'Volatility']]\n",
        "        candidates = candidates.sort_values('Confidence', ascending=False)\n",
        "\n",
        "        candidates.to_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\", index=False)\n",
        "        logging.info(f\"Cell 5: Confidence threshold: {confidence_threshold}, Volatility: {candidates['Volatility'].mean():.4f}\")\n",
        "        logging.info(f\"Cell 5: Confidence stats: {candidates['Confidence'].describe().to_dict()}\")\n",
        "        logging.info(f\"Cell 5: Generated {len(candidates)} trade candidates, Excluded tickers: {excluded_tickers}\")\n",
        "        logging.info(f\"Cell 5: Trade candidates generated, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 5: Trade candidates generated, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return candidates\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 5: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "predictions_df = pd.read_csv(f\"{CONFIG['export_dir']}/predictions.csv\")\n",
        "trade_candidates_df = generate_trade_candidates(predictions_df)"
      ],
      "metadata": {
        "id": "L9qcgFUBMNNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5Z: Debug Trade Candidates\n",
        "def debug_trade_candidates(predictions_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        logging.info(f\"Cell 5Z: Predictions shape: {predictions_df.shape}\")\n",
        "        confidence_col = 'Adjusted_Confidence' if 'Adjusted_Confidence' in predictions_df.columns else 'Confidence'\n",
        "        df = predictions_df.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        confidence_stats = {\n",
        "            'mean': df[confidence_col].mean(),\n",
        "            'min': df[confidence_col].min(),\n",
        "            'max': df[confidence_col].max(),\n",
        "            '25%': df[confidence_col].quantile(0.25),\n",
        "            '50%': df[confidence_col].median(),\n",
        "            '75%': df[confidence_col].quantile(0.75)\n",
        "        }\n",
        "        logging.info(f\"Cell 5Z: Confidence stats: {confidence_stats}\")\n",
        "        logging.info(f\"Cell 5Z: Predictions: {df[['Ticker', 'Date', confidence_col]].to_dict()}\")\n",
        "\n",
        "        confidence_trend = df.groupby(df['Date'].dt.date)[confidence_col].mean().reset_index()\n",
        "        logging.info(f\"Cell 5Z: Confidence trend: {confidence_trend.to_dict()}\")\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        loss_data = pd.read_sql_query(\"SELECT Ticker, Loss_Date FROM loss_tracker\", conn)\n",
        "        conn.close()\n",
        "        logging.info(f\"Cell 5Z: Exclusion history: {loss_data.to_dict()}\")\n",
        "\n",
        "        logging.info(f\"Cell 5Z: Trade candidate debug completed, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 5Z: Trade candidate debug completed, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 5Z: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "predictions_df = pd.read_csv(f\"{CONFIG['export_dir']}/predictions.csv\")\n",
        "debug_trade_candidates(predictions_df)"
      ],
      "metadata": {
        "id": "XDzC3j5uqhTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Telegram Alerts\n",
        "def send_telegram_alert(candidates_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if candidates_df.empty:\n",
        "            logging.warning(\"Cell 6: No candidates for Telegram alert\")\n",
        "            print(\"Cell 6: No candidates for Telegram alert\")\n",
        "            return\n",
        "\n",
        "        bot_token = '7779970479:AAFJFop5XrTe7_dP1iGDoGVM-bdWNyYso8E'\n",
        "        chat_id = '1591809098'\n",
        "        for _, row in candidates_df.iterrows():\n",
        "            if row['Type'] == 'Buy':\n",
        "                message = (\n",
        "                    f\" Trade Alert \\n\"\n",
        "                    f\"Ticker: {row['Ticker']}\\n\"\n",
        "                    f\"Date: {row['Date']}\\n\"\n",
        "                    f\"Action: Buy\\n\"\n",
        "                    f\"Entry Price: ${row['Entry_Price']:.2f}\\n\"\n",
        "                    f\"Target Price: ${row['Target_Price']:.2f}\\n\"\n",
        "                    f\"Stop Loss: ${row['Stop_Loss']:.2f}\\n\"\n",
        "                    f\"Confidence: {row['Confidence']:.2%}\"\n",
        "                )\n",
        "                url = f\"https://api.telegram.org/bot{bot_token}/sendMessage?chat_id={chat_id}&text={message}\"\n",
        "                response = requests.get(url)\n",
        "                response.raise_for_status()\n",
        "                logging.info(f\"Cell 6: Telegram alert sent for {row['Ticker']}.\")\n",
        "                print(f\"Cell 6: Telegram alert sent for {row['Ticker']}.\")\n",
        "                time.sleep(0.2)\n",
        "        logging.info(f\"Cell 6: Telegram alerts completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 6: Telegram alerts completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 6: Failed: {str(e)}\")\n",
        "        raise"
      ],
      "metadata": {
        "id": "mLv9vUOhMNK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Backtest Strategy\n",
        "def backtest_strategy(data, trade_candidates, target_multiplier=0.8, stop_loss_multiplier=2.0):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if trade_candidates.empty or data.empty:\n",
        "            raise ValueError(\"Cell 7: Empty trade candidates or data\")\n",
        "\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        candidates = trade_candidates.copy()\n",
        "        candidates['Date'] = pd.to_datetime(candidates['Date'], utc=True)\n",
        "\n",
        "        results = []\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/stock_data.db\")\n",
        "        for _, candidate in candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            entry_date = pd.to_datetime(candidate['Date'], utc=True)\n",
        "            entry_price = candidate['Close']\n",
        "\n",
        "            start_date = entry_date - pd.Timedelta(days=30)\n",
        "            end_date = entry_date\n",
        "            ticker_data = df[(df['Ticker'] == ticker) & (df['Date'] >= start_date) & (df['Date'] <= end_date)].sort_values('Date')\n",
        "\n",
        "            if len(ticker_data) < 14:\n",
        "                logging.warning(f\"Cell 7: Insufficient data for {ticker} from {start_date} to {end_date}, attempting yfinance fetch\")\n",
        "                try:\n",
        "                    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "                    def fetch_yfinance(ticker, start, end):\n",
        "                        return yf.download(ticker, start=start, end=end + pd.Timedelta(days=1), progress=False, auto_adjust=True)\n",
        "\n",
        "                    new_data = fetch_yfinance(ticker, start_date, end_date)\n",
        "                    if not new_data.empty:\n",
        "                        new_data = new_data.reset_index().rename(columns={'Datetime': 'Date'})\n",
        "                        new_data['Date'] = pd.to_datetime(new_data['Date'], utc=True)\n",
        "                        new_data['Ticker'] = ticker\n",
        "                        ticker_data = pd.concat([ticker_data, new_data], ignore_index=True).drop_duplicates(subset=['Date', 'Ticker'])\n",
        "                        existing_rows = pd.read_sql_query(\n",
        "                            f\"SELECT Date, Ticker FROM raw_stock_data WHERE Ticker = ?\",\n",
        "                            conn, params=[ticker]\n",
        "                        )\n",
        "                        existing_rows['Date'] = pd.to_datetime(existing_rows['Date'], utc=True)\n",
        "                        existing_set = set(existing_rows[['Date', 'Ticker']].itertuples(index=False, name=None))\n",
        "                        new_rows = new_data[~new_data[['Date', 'Ticker']].apply(tuple, axis=1).isin(existing_set)]\n",
        "                        if not new_rows.empty:\n",
        "                            new_rows.to_sql('raw_stock_data', conn, if_exists='append', index=False, method='multi')\n",
        "                            logging.info(f\"Cell 7: Cached {len(new_rows)} rows for {ticker}\")\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Cell 7: Failed to fetch additional data for {ticker}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if len(ticker_data) < 14:\n",
        "                logging.warning(f\"Cell 7: Insufficient data for {ticker} from {start_date} to {end_date}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                ticker_data['ATR'] = ta.atr(ticker_data['High'], ticker_data['Low'], ticker_data['Close'], length=14).fillna(1.0)\n",
        "                atr = ticker_data[ticker_data['Date'] <= entry_date]['ATR'].iloc[-1] if not ticker_data[ticker_data['Date'] <= entry_date].empty else 1.0\n",
        "                atr = atr if pd.notna(atr) else 1.0\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Cell 7: ATR calculation failed for {ticker}: {str(e)}, using default 1.0\")\n",
        "                atr = 1.0\n",
        "\n",
        "            target_price = entry_price + atr * target_multiplier\n",
        "            stop_loss_price = entry_price - atr * stop_loss_multiplier\n",
        "\n",
        "            outcome = 'Loss'\n",
        "            exit_price = stop_loss_price\n",
        "            exit_date = entry_date\n",
        "\n",
        "            future_data = df[(df['Ticker'] == ticker) & (df['Date'] > entry_date)].sort_values('Date')\n",
        "            if future_data.empty:\n",
        "                logging.warning(f\"Cell 7: No future data for {ticker} after {entry_date}, using historical simulation\")\n",
        "                future_data = ticker_data[ticker_data['Date'] <= entry_date].iloc[-10:]\n",
        "\n",
        "            for _, row in future_data.iterrows():\n",
        "                if row['High'] >= target_price:\n",
        "                    outcome = 'Win'\n",
        "                    exit_price = target_price\n",
        "                    exit_date = pd.to_datetime(row['Date'], utc=True)\n",
        "                    break\n",
        "                if row['Low'] <= stop_loss_price:\n",
        "                    outcome = 'Loss'\n",
        "                    exit_price = stop_loss_price\n",
        "                    exit_date = pd.to_datetime(row['Date'], utc=True)\n",
        "                    break\n",
        "\n",
        "            profit = exit_price - entry_price\n",
        "            results.append({\n",
        "                'Ticker': ticker,\n",
        "                'Entry_Date': entry_date,\n",
        "                'Entry_Price': entry_price,\n",
        "                'Exit_Date': exit_date,\n",
        "                'Exit_Price': exit_price,\n",
        "                'Outcome': outcome,\n",
        "                'Profit': profit,\n",
        "                'ATR': atr,\n",
        "                'Target_Price': target_price,\n",
        "                'Stop_Loss': stop_loss_price\n",
        "            })\n",
        "\n",
        "        conn.close()\n",
        "        results_df = pd.DataFrame(results)\n",
        "        if results_df.empty:\n",
        "            logging.warning(\"Cell 7: No trades executed\")\n",
        "            return results_df\n",
        "\n",
        "        # Add losses to tracker\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"CREATE TABLE IF NOT EXISTS loss_tracker (Ticker TEXT, Loss_Date TEXT)\")\n",
        "        for _, row in results_df[results_df['Outcome'] == 'Loss'].iterrows():\n",
        "            cursor.execute(\"INSERT OR IGNORE INTO loss_tracker (Ticker, Loss_Date) VALUES (?, ?)\", (row['Ticker'], str(row['Exit_Date'])))\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "        win_rate = (results_df['Outcome'] == 'Win').mean() * 100\n",
        "        reward_risk = results_df[results_df['Outcome'] == 'Win']['Profit'].sum() / abs(results_df[results_df['Outcome'] == 'Loss']['Profit'].sum() + 1e-6)\n",
        "\n",
        "        results_df.to_csv(f\"{CONFIG['export_dir']}/backtest_results.csv\", index=False)\n",
        "        logging.info(f\"Cell 7: Backtest completed, {len(results_df)} trades, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7: Backtest completed, {len(results_df)} trades, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return results_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "backtest_results = backtest_strategy(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "id": "R8QSKwVVMNIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7Z: Backtest Debug\n",
        "def debug_backtest(data, trade_candidates):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        candidates = trade_candidates.copy()\n",
        "        candidates['Date'] = pd.to_datetime(candidates['Date'], utc=True)\n",
        "\n",
        "        logging.info(f\"Cell 7Z: Trade candidates shape: {candidates.shape}\")\n",
        "        logging.info(f\"Cell 7Z: Data shape: {df.shape}\")\n",
        "\n",
        "        for _, candidate in candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            entry_date = candidate['Date']\n",
        "            entry_price = candidate['Close']\n",
        "\n",
        "            ticker_data = df[(df['Ticker'] == ticker) & (df['Date'] >= entry_date)].sort_values('Date')\n",
        "            atr = ticker_data['ATR'].iloc[0] if not ticker_data.empty and 'ATR' in ticker_data.columns else 1.0\n",
        "            adx = ticker_data['ADX'].iloc[0] if not ticker_data.empty and 'ADX' in ticker_data.columns else 0.0\n",
        "            logging.info(f\"Cell 7Z: Ticker {ticker}, Date {entry_date}, Entry Price {entry_price:.2f}, ATR {atr:.2f}, ADX {adx:.2f}, Ticker Data Rows {len(ticker_data)}\")\n",
        "\n",
        "        logging.info(f\"Cell 7Z: Backtest debug completed, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7Z: Backtest debug completed, Shape: {candidates.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7Z: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "debug_backtest(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "id": "eHQA61QcmmD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7Z2: Win Rate Debugging\n",
        "def debug_win_rate(data, trade_candidates):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        candidates = trade_candidates.copy()\n",
        "        candidates['Date'] = pd.to_datetime(candidates['Date'], utc=True)\n",
        "\n",
        "        logging.info(f\"Cell 7Z2: Trade candidates shape: {candidates.shape}\")\n",
        "        logging.info(f\"Cell 7Z2: Data shape: {df.shape}\")\n",
        "        logging.info(f\"Cell 7Z2: Data date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "\n",
        "        for _, candidate in candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            entry_date = candidate['Date']\n",
        "            entry_price = candidate['Close']\n",
        "\n",
        "            ticker_data = df[(df['Ticker'] == ticker) & (df['Date'] >= entry_date)].sort_values('Date')\n",
        "            atr = ticker_data['ATR'].iloc[0] if not ticker_data.empty and 'ATR' in ticker_data.columns else 1.0\n",
        "            adx = ticker_data['ADX'].iloc[0] if not ticker_data.empty and 'ADX' in ticker_data.columns else 0.0\n",
        "            logging.info(f\"Cell 7Z2: Ticker {ticker}, Date {entry_date}, Entry Price {entry_price:.2f}, ATR {atr:.2f}, ADX {adx:.2f}, Ticker Data Rows {len(ticker_data)}\")\n",
        "\n",
        "        # Check loss tracker\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/loss_tracker.db\")\n",
        "        loss_data = pd.read_sql_query(\"SELECT Ticker, Loss_Date FROM loss_tracker\", conn)\n",
        "        conn.close()\n",
        "        logging.info(f\"Cell 7Z2: Loss tracker: {loss_data.to_dict()}\")\n",
        "\n",
        "        logging.info(f\"Cell 7Z2: Win rate debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7Z2: Win rate debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7Z2: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "debug_win_rate(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "id": "mA0pk7pADKue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7Z3: Backtest Data Debug\n",
        "def debug_backtest_data(data, trade_candidates):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        candidates = trade_candidates.copy()\n",
        "        candidates['Date'] = pd.to_datetime(candidates['Date'], utc=True)\n",
        "\n",
        "        logging.info(f\"Cell 7Z3: Trade candidates shape: {candidates.shape}\")\n",
        "        logging.info(f\"Cell 7Z3: Data shape: {df.shape}\")\n",
        "        logging.info(f\"Cell 7Z3: Data date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
        "\n",
        "        for _, candidate in candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            entry_date = pd.to_datetime(candidate['Date'], utc=True)\n",
        "            entry_price = candidate['Close']\n",
        "\n",
        "            start_date = pd.to_datetime('2025-06-20', utc=True)\n",
        "            end_date = pd.to_datetime('2025-07-03', utc=True)\n",
        "            ticker_data = df[(df['Ticker'] == ticker) & (df['Date'] >= start_date) & (df['Date'] <= end_date)].sort_values('Date')\n",
        "\n",
        "            try:\n",
        "                ticker_data['ATR'] = ta.atr(ticker_data['High'], ticker_data['Low'], ticker_data['Close'], length=14)\n",
        "                atr = ticker_data[ticker_data['Date'] >= entry_date]['ATR'].iloc[0] if not ticker_data[ticker_data['Date'] >= entry_date].empty else 1.0\n",
        "                atr = atr if pd.notna(atr) else 1.0\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Cell 7Z3: ATR calculation failed for {ticker}: {str(e)}, using default 1.0\")\n",
        "                atr = 1.0\n",
        "\n",
        "            logging.info(f\"Cell 7Z3: Ticker {ticker}, Date {entry_date}, Entry Price {entry_price:.2f}, ATR {atr:.2f}, Data Rows {len(ticker_data)}\")\n",
        "\n",
        "        logging.info(f\"Cell 7Z3: Backtest data debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7Z3: Backtest data debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7Z3: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "trade_candidates_df = pd.read_csv(f\"{CONFIG['export_dir']}/trade_candidates.csv\")\n",
        "debug_backtest_data(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "id": "BElKOe04THJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7ZZ: Temporary Backtesting Input Debug\n",
        "def debug_backtesting_inputs(data, trade_candidates):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        data['Date'] = pd.to_datetime(data['Date'], utc=True)\n",
        "        trade_candidates['Date'] = pd.to_datetime(trade_candidates['Date'], utc=True)\n",
        "        debug_results = []\n",
        "\n",
        "        logging.info(f\"Cell 7ZZ: Trade candidates: {trade_candidates[['Ticker', 'Date', 'Close']].to_dict()}\")\n",
        "        logging.info(f\"Cell 7ZZ: Data date range: {data['Date'].min()} to {data['Date'].max()}\")\n",
        "\n",
        "        for _, candidate in trade_candidates.iterrows():\n",
        "            ticker = candidate['Ticker']\n",
        "            date = candidate['Date']\n",
        "            ticker_data = data[(data['Ticker'] == ticker) & (data['Date'] > date)].head(5)\n",
        "            debug_results.append({\n",
        "                'Ticker': ticker,\n",
        "                'Candidate_Date': date,\n",
        "                'Data_Available': len(ticker_data) > 0,\n",
        "                'Ticker_Data_Dates': list(ticker_data['Date']) if not ticker_data.empty else []\n",
        "            })\n",
        "\n",
        "        debug_df = pd.DataFrame(debug_results)\n",
        "        debug_df.to_csv(f\"{CONFIG['export_dir']}/backtest_inputs_debug.csv\", index=False)\n",
        "        logging.info(f\"Cell 7ZZ: Backtesting inputs debug completed, Shape: {debug_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 7ZZ: Backtesting inputs debug completed, Shape: {debug_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return debug_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 7ZZ: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "debug_backtest_inputs = debug_backtesting_inputs(processed_data, trade_candidates_df)"
      ],
      "metadata": {
        "id": "gp1tMwHIn1Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Track Performance\n",
        "def track_performance(backtest_results):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if backtest_results.empty:\n",
        "            logging.warning(\"Cell 8: No backtest results to track\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        df = backtest_results.copy()\n",
        "        df['Returns'] = df['Profit'] / df['Entry_Price']\n",
        "\n",
        "        # Withhold 40% for taxes on wins\n",
        "        net_returns = df['Returns'].copy()\n",
        "        net_returns[df['Outcome'] == 'Win'] *= 0.6  # 60% retained\n",
        "        net_profit = net_returns.sum()\n",
        "\n",
        "        win_rate = (df['Outcome'] == 'Win').mean() * 100\n",
        "        reward_risk = df[df['Outcome'] == 'Win']['Profit'].sum() * 0.6 / abs(df[df['Outcome'] == 'Loss']['Profit'].sum() + 1e-6)\n",
        "        sharpe_ratio = net_returns.mean() / (net_returns.std() + 1e-6) * np.sqrt(252)\n",
        "        cumulative_returns = (1 + net_returns).cumprod()\n",
        "        max_drawdown = (cumulative_returns.cummax() - cumulative_returns).max()\n",
        "\n",
        "        performance_df = df.copy()\n",
        "        performance_df['Win_Rate'] = win_rate\n",
        "        performance_df['Reward_Risk'] = reward_risk\n",
        "        performance_df['Sharpe'] = sharpe_ratio\n",
        "        performance_df['Max_Drawdown'] = max_drawdown\n",
        "        performance_df['Net_Profit'] = net_profit\n",
        "\n",
        "        performance_df.to_csv(f\"{CONFIG['export_dir']}/performance_tracker.csv\", index=False)\n",
        "        logging.info(f\"Cell 8: Performance tracked, Shape: {performance_df.shape}, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2f}%, Net Profit: {net_profit:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 8: Performance tracked, Shape: {performance_df.shape}, Win rate: {win_rate:.2f}%, Reward:Risk: {reward_risk:.2f}, Sharpe: {sharpe_ratio:.2f}, Max Drawdown: {max_drawdown:.2f}%, Net Profit: {net_profit:.2f}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return performance_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 8: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "backtest_results = pd.read_csv(f\"{CONFIG['export_dir']}/backtest_results.csv\")\n",
        "performance_df = track_performance(backtest_results)"
      ],
      "metadata": {
        "id": "rBfzCKGQMezV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Retrain Model with Feedback\n",
        "def retrain_model(data, performance_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "\n",
        "        if not performance_df.empty:\n",
        "            performance_df['Date'] = pd.to_datetime(performance_df['Entry_Date'], utc=True)\n",
        "            df = df.merge(performance_df[['Ticker', 'Date', 'Outcome']], on=['Ticker', 'Date'], how='left')\n",
        "            logging.info(f\"Cell 9: Merged shape: {df.shape}, Outcome nulls: {df['Outcome'].isna().sum()}\")\n",
        "\n",
        "        features = [col for col in df.columns if col not in ['Date', 'Ticker', 'Open', 'High', 'Low', 'Close', 'Volume', 'Outcome']]\n",
        "        df['Next_Close'] = df.groupby('Ticker')['Close'].shift(-1)\n",
        "        df['Target'] = df.apply(\n",
        "            lambda row: 1 if pd.notna(row['Next_Close']) and row['Next_Close'] > row['Close'] * 1.01 else 0,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Use backtest outcome as target if available\n",
        "        if 'Outcome' in df.columns:\n",
        "            df['Outcome'] = df['Outcome'].map({'Win': 1, 'Loss': 0})\n",
        "            df['Target'] = df['Outcome'].where(df['Outcome'].notna(), df['Target'])\n",
        "\n",
        "        df = df.drop(columns=['Next_Close']).dropna(subset=['Target'] + features)\n",
        "        logging.info(f\"Cell 9: Target distribution: {df['Target'].value_counts().to_dict()}\")\n",
        "\n",
        "        X = df[features].fillna(0)\n",
        "        y = df['Target'].astype(int)\n",
        "\n",
        "        def xgb_objective(trial):\n",
        "            xgb_params = {\n",
        "                'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
        "                'max_depth': trial.suggest_int('max_depth', 3, 7),\n",
        "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05)\n",
        "            }\n",
        "            xgb_model = XGBClassifier(**xgb_params, random_state=42, scale_pos_weight=(y==0).sum()/(y==1).sum() + 1e-6)\n",
        "            xgb_model.fit(X, y)\n",
        "            return xgb_model.score(X, y)\n",
        "\n",
        "        xgb_study = optuna.create_study(direction='maximize')\n",
        "        xgb_study.optimize(xgb_objective, n_trials=5)\n",
        "        xgb_model = XGBClassifier(**xgb_study.best_params, random_state=42, scale_pos_weight=(y==0).sum()/(y==1).sum() + 1e-6)\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        xgb_model.fit(X_scaled, y)\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_model.pkl\", 'wb') as f:\n",
        "            pickle.dump(xgb_model, f)\n",
        "        logging.info(f\"Cell 9: Saved {CONFIG['export_dir']}/retrained_model.pkl\")\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_scaler.pkl\", 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        logging.info(f\"Cell 9: Saved {CONFIG['export_dir']}/retrained_scaler.pkl\")\n",
        "        logging.info(f\"Cell 9: Model retrained, Best params: {xgb_study.best_params}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 9: Model retrained, Best params: {xgb_study.best_params}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return xgb_model, scaler\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 9: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "performance_df = pd.read_csv(f\"{CONFIG['export_dir']}/performance_tracker.csv\")\n",
        "retrained_model, retrained_scaler = retrain_model(processed_data, performance_df)"
      ],
      "metadata": {
        "id": "v_QulAIrMeun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9Z: Retraining Debug\n",
        "def debug_retraining(data, performance_df):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        logging.info(f\"Cell 9Z: Data shape: {data.shape}, Performance shape: {performance_df.shape}\")\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "\n",
        "        if not performance_df.empty:\n",
        "            performance_df['Entry_Date'] = pd.to_datetime(performance_df['Entry_Date'], utc=True)\n",
        "            df = df.merge(performance_df[['Ticker', 'Entry_Date', 'Outcome']], left_on=['Ticker', 'Date'], right_on=['Ticker', 'Entry_Date'], how='left')\n",
        "            logging.info(f\"Cell 9Z: Merged shape: {df.shape}, Outcome nulls: {df['Outcome'].isna().sum()}\")\n",
        "\n",
        "        df['Next_Close'] = df.groupby('Ticker')['Close'].shift(-1)\n",
        "        logging.info(f\"Cell 9Z: Next_Close nulls: {df['Next_Close'].isna().sum()}\")\n",
        "\n",
        "        df['Target'] = (df['Next_Close'] > df['Close'] * 1.01).astype(int).fillna(0)\n",
        "        logging.info(f\"Cell 9Z: Target distribution: {df['Target'].value_counts().to_dict()}\")\n",
        "\n",
        "        logging.info(f\"Cell 9Z: Retraining debug completed, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 9Z: Retraining debug completed, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 9Z: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "performance_df = pd.read_csv(f\"{CONFIG['export_dir']}/performance_tracker.csv\")\n",
        "debug_retraining(processed_data, performance_df)"
      ],
      "metadata": {
        "id": "C1dR1wQ-mo08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Fetch Intraday Data\n",
        "def fetch_intraday_data(tickers, interval=CONFIG['intraday_interval'], lookback_hours=CONFIG['intraday_lookback_hours']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/stock_data.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''CREATE TABLE IF NOT EXISTS intraday_data (\n",
        "            Date TEXT, Ticker TEXT, Open REAL, High REAL, Low REAL, Close REAL, Volume INTEGER,\n",
        "            PRIMARY KEY (Date, Ticker)\n",
        "        )''')\n",
        "\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=5)  # Sufficient for intraday lookback\n",
        "        cached_data = pd.read_sql_query(\n",
        "            f\"SELECT * FROM intraday_data WHERE Date >= ? AND Date <= ? AND Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "            conn, params=[start_date.strftime('%Y-%m-%d %H:%M:%S'), end_date.strftime('%Y-%m-%d %H:%M:%S')] + tickers\n",
        "        )\n",
        "        cached_data['Date'] = pd.to_datetime(cached_data['Date'], utc=True)\n",
        "        cached_tickers = set(cached_data['Ticker'])\n",
        "        new_tickers = [t for t in tickers if t not in cached_tickers]\n",
        "\n",
        "        if len(cached_data) >= len(tickers) * lookback_hours * 0.7 and not new_tickers:\n",
        "            logging.info(f\"Cell 10: Loaded {len(cached_data)} intraday rows from cache\")\n",
        "            df = cached_data\n",
        "        else:\n",
        "            batch_size = 5\n",
        "            all_data = [cached_data] if not cached_data.empty else []\n",
        "            for i in range(0, len(new_tickers), batch_size):\n",
        "                batch_tickers = new_tickers[i:i + batch_size]\n",
        "                try:\n",
        "                    @retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "                    def fetch_yfinance(tickers, start, end, interval):\n",
        "                        return yf.download(tickers, start=start, end=end, interval=interval, progress=False, auto_adjust=True)\n",
        "\n",
        "                    data = fetch_yfinance(batch_tickers, start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'), interval)\n",
        "                    if data.empty:\n",
        "                        raise ValueError(f\"No intraday data for {batch_tickers}\")\n",
        "                    if isinstance(data.columns, pd.MultiIndex):\n",
        "                        data.columns = [f\"{col[0]}_{col[1]}\" for col in data.columns]\n",
        "                    data = data.reset_index().rename(columns={'Datetime': 'Date'})\n",
        "                    data['Date'] = pd.to_datetime(data['Date'], utc=True)\n",
        "                    melted = data.melt(id_vars=['Date'], var_name='Metric', value_name='Value')\n",
        "                    melted['Ticker'] = melted['Metric'].str.split('_').str[-1]\n",
        "                    melted['Metric'] = melted['Metric'].str.split('_').str[0]\n",
        "                    df_batch = melted.pivot_table(index=['Date', 'Ticker'], columns='Metric', values='Value').reset_index()\n",
        "                    df_batch.columns.name = None\n",
        "                    df_batch.columns = [col.capitalize() for col in df_batch.columns]\n",
        "                    df_batch = df_batch.drop_duplicates(subset=['Date', 'Ticker'], keep='last')\n",
        "                    all_data.append(df_batch)\n",
        "                    logging.info(f\"Cell 10: Fetched intraday data for {batch_tickers}, Shape: {df_batch.shape}\")\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Cell 10: yfinance failed for {batch_tickers}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            df = pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
        "            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')\n",
        "            df = df.dropna(subset=['Date', 'Close', 'Ticker'])\n",
        "            df = df.sort_values(['Ticker', 'Date']).drop_duplicates(subset=['Date', 'Ticker'], keep='last').reset_index(drop=True)\n",
        "\n",
        "            if not df.empty:\n",
        "                existing_rows = pd.read_sql_query(\n",
        "                    f\"SELECT Date, Ticker FROM intraday_data WHERE Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "                    conn, params=tickers\n",
        "                )\n",
        "                existing_rows['Date'] = pd.to_datetime(existing_rows['Date'], utc=True)\n",
        "                existing_set = set(existing_rows[['Date', 'Ticker']].itertuples(index=False, name=None))\n",
        "                new_rows = df[~df[['Date', 'Ticker']].apply(tuple, axis=1).isin(existing_set)]\n",
        "                if not new_rows.empty:\n",
        "                    new_rows.to_sql('intraday_data', conn, if_exists='append', index=False, method='multi')\n",
        "                    logging.info(f\"Cell 10: Cached {len(new_rows)} new intraday rows\")\n",
        "\n",
        "        conn.close()\n",
        "        df.to_csv(f\"{CONFIG['export_dir']}/intraday_data.csv\", index=False)\n",
        "        logging.info(f\"Cell 10: Intraday data fetched, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 10: Intraday data fetched, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 10: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "intraday_data = fetch_intraday_data(CONFIG['tickers'])"
      ],
      "metadata": {
        "id": "J1d08mqgMekw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10Z: Temporary Intraday Data Debug\n",
        "def debug_intraday_data(tickers, interval=CONFIG['intraday_interval'], lookback_hours=CONFIG['intraday_lookback_hours']):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        end_date = pd.to_datetime('today', utc=True)\n",
        "        test_date = end_date\n",
        "        attempts = 0\n",
        "        max_attempts = 5\n",
        "        while attempts < max_attempts:\n",
        "            start_date = test_date - pd.Timedelta(hours=lookback_hours)\n",
        "            test_df = yf.download(tickers[:2], start=start_date, end=test_date, interval=interval, progress=False)\n",
        "            logging.info(f\"Cell 10Z: Test fetch for {tickers[:2]}, Start: {start_date}, End: {test_date}, Rows: {len(test_df)}\")\n",
        "            if not test_df.empty:\n",
        "                break\n",
        "            test_date -= pd.Timedelta(days=1)\n",
        "            attempts += 1\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/stock_data.db\")\n",
        "        cached_data = pd.read_sql_query(\n",
        "            f\"SELECT * FROM intraday_data WHERE Ticker IN ({','.join(['?']*len(tickers))})\",\n",
        "            conn, params=tickers\n",
        "        )\n",
        "        cached_data['Date'] = pd.to_datetime(cached_data['Date'], utc=True)\n",
        "        logging.info(f\"Cell 10Z: Cached data shape: {cached_data.shape}, Date range: {cached_data['Date'].min()} to {cached_data['Date'].max()}\")\n",
        "        conn.close()\n",
        "\n",
        "        debug_df = pd.DataFrame({'Test_Date': [test_date], 'Start_Date': [start_date], 'Rows_Fetched': [len(test_df)]})\n",
        "        debug_df.to_csv(f\"{CONFIG['export_dir']}/intraday_debug.csv\", index=False)\n",
        "        logging.info(f\"Cell 10Z: Intraday debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 10Z: Intraday debug completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        return debug_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 10Z: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "debug_intraday_results = debug_intraday_data(CONFIG['tickers'])"
      ],
      "metadata": {
        "id": "IzzePphznwsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Diagnostics\n",
        "def run_diagnostics():\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        files = os.listdir(f\"{CONFIG['export_dir']}\")\n",
        "        logging.info(f\"Cell 11: Files: {files}\")\n",
        "\n",
        "        processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "        nan_counts = processed_data.isna().sum()\n",
        "        logging.info(f\"Cell 11: Processed data shape: {processed_data.shape}, NaNs: {nan_counts[nan_counts > 0].to_dict()}\")\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/model_rf.pkl\", 'rb') as f:\n",
        "            pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/model_xgb.pkl\", 'rb') as f:\n",
        "            pickle.load(f)\n",
        "        logging.info(\"Cell 11: Models loaded successfully\")\n",
        "\n",
        "        sentiment_scores = {}\n",
        "        for ticker in CONFIG['tickers']:\n",
        "            try:\n",
        "                sentiment_scores[ticker] = np.random.uniform(0, 1)  # Placeholder\n",
        "                logging.info(f\"Cell 11: Sentiment score for {ticker}: {sentiment_scores[ticker]:.2f}\")\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Cell 11: Failed to get sentiment for {ticker}: {str(e)}\")\n",
        "                sentiment_scores[ticker] = 0.5\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/sentiment_scores.json\", 'w') as f:\n",
        "            json.dump(sentiment_scores, f)\n",
        "        logging.info(f\"Cell 11: Sentiment scores saved: {sentiment_scores}\")\n",
        "        logging.info(f\"Cell 11: Diagnostics completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 11: Diagnostics completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 11: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "run_diagnostics()"
      ],
      "metadata": {
        "id": "nYnxTK43MeiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Pipeline Orchestration\n",
        "def monitor_intraday(intraday_data, last_alert_time):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        if intraday_data.empty:\n",
        "            logging.warning(\"Cell 12: Intraday data is empty, skipping monitoring\")\n",
        "            return last_alert_time\n",
        "\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_model.pkl\", 'rb') as f:\n",
        "            model = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/retrained_scaler.pkl\", 'rb') as f:\n",
        "            scaler = pickle.load(f)\n",
        "        with open(f\"{CONFIG['export_dir']}/selected_features.pkl\", 'rb') as f:\n",
        "            selected_features = pickle.load(f)\n",
        "\n",
        "        processed_intraday = calculate_technicals(intraday_data, is_intraday=True)\n",
        "\n",
        "        X = processed_intraday[selected_features].fillna(0)\n",
        "        X_scaled = scaler.transform(X)\n",
        "\n",
        "        confidences = model.predict_proba(X_scaled)[:, 1]\n",
        "        processed_intraday['Confidence'] = confidences\n",
        "\n",
        "        signals = processed_intraday[processed_intraday['Confidence'] >= CONFIG['confidence_threshold']]\n",
        "        signals = signals[['Ticker', 'Date', 'Close', 'Confidence']]\n",
        "\n",
        "        signals.to_csv(f\"{CONFIG['export_dir']}/intraday_signals.csv\", index=False)\n",
        "        logging.info(f\"Cell 12: Generated {len(signals)} intraday signals\")\n",
        "\n",
        "        return pd.to_datetime('now', utc=True)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 12: Monitor intraday failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def run_pipeline():\n",
        "    start_time = time.time()\n",
        "    retries = 3\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            initialize_indicator_weights(CONFIG['tickers'])\n",
        "            transformed_data = fetch_and_transform_data(CONFIG['tickers'])\n",
        "            processed_data = calculate_technicals(transformed_data)\n",
        "            scaler, selected_features, predictions_df = train_model(processed_data)\n",
        "            trade_candidates_df = generate_trade_candidates(predictions_df)\n",
        "            backtest_results = backtest_strategy(processed_data, trade_candidates_df)\n",
        "            performance_df = track_performance(backtest_results)\n",
        "            retrained_model, retrained_scaler = retrain_model(processed_data, performance_df)\n",
        "            intraday_data = fetch_intraday_data(CONFIG['tickers'])\n",
        "            last_alert_time = None\n",
        "            if not intraday_data.empty:\n",
        "                last_alert_time = monitor_intraday(intraday_data, None)\n",
        "            logging.info(f\"Cell 12: Pipeline completed, Time: {time.time() - start_time:.2f}s\")\n",
        "            print(f\"Cell 12: Pipeline completed, Time: {time.time() - start_time:.2f}s\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Cell 12: Attempt {attempt+1} failed: {str(e)}\")\n",
        "            if attempt == retries - 1:\n",
        "                logging.error(f\"Cell 12: Pipeline failed after {retries} attempts: {str(e)}\")\n",
        "                print(f\"Cell 12: Pipeline failed: {str(e)}\")\n",
        "                raise\n",
        "            time.sleep(5)\n",
        "\n",
        "# Execute\n",
        "run_pipeline()"
      ],
      "metadata": {
        "id": "zj4m8FLxMMj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Schedule Pipeline\n",
        "def schedule_pipeline():\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        current_time = pd.to_datetime('now', utc=True)\n",
        "        market_open = current_time.replace(hour=9, minute=30, second=0, microsecond=0)\n",
        "        market_close = current_time.replace(hour=16, minute=0, second=0, microsecond=0)\n",
        "\n",
        "        if current_time.weekday() < 5 and market_open <= current_time <= market_close:\n",
        "            run_pipeline()\n",
        "        else:\n",
        "            logging.info(\"Cell 13: Market closed, skipping pipeline run\")\n",
        "\n",
        "        logging.info(f\"Cell 13: Schedule check completed, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 13: Schedule check completed, Time: {time.time() - start_time:.2f}s\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 13: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "schedule_pipeline()"
      ],
      "metadata": {
        "id": "1JOZzCwptyyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Modular Indicator Framework and Debugging\n",
        "def load_indicator_config(config_path=f\"{CONFIG['export_dir']}/indicator_config.json\"):\n",
        "    default_config = {\n",
        "        \"indicators\": {\n",
        "            \"RSI_lag1\": {\"enabled\": True, \"params\": {\"length\": 14, \"shift\": 1}},\n",
        "            \"Volatility_5\": {\"enabled\": True, \"params\": {\"window\": 5}},\n",
        "            \"CMF\": {\"enabled\": True, \"params\": {\"length\": 20}},\n",
        "            \"ADX\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"MACD\": {\"enabled\": True, \"params\": {\"fast\": 12, \"slow\": 26, \"signal\": 9}},\n",
        "            \"Signal\": {\"enabled\": True, \"params\": {\"fast\": 12, \"slow\": 26, \"signal\": 9}},\n",
        "            \"MACD_Histogram\": {\"enabled\": True, \"params\": {\"fast\": 12, \"slow\": 26, \"signal\": 9}},\n",
        "            \"RSI\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"OBV\": {\"enabled\": True, \"params\": {}},\n",
        "            \"ATR\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"Volume_MA5\": {\"enabled\": True, \"params\": {\"length\": 5}},\n",
        "            \"RSI_M\": {\"enabled\": True, \"params\": {\"length\": 14}},\n",
        "            \"Stoch_RSI\": {\"enabled\": True, \"params\": {\"length\": 14, \"smooth_k\": 3, \"smooth_d\": 3}},\n",
        "            \"Volume_Oscillator\": {\"enabled\": True, \"params\": {\"short_length\": 5, \"long_length\": 20}},\n",
        "            \"KC_Upper\": {\"enabled\": True, \"params\": {\"length\": 20, \"scalar\": 2}}\n",
        "        }\n",
        "    }\n",
        "    try:\n",
        "        if os.path.exists(config_path):\n",
        "            with open(config_path, 'r') as f:\n",
        "                config = json.load(f)\n",
        "            logging.info(f\"Cell 14: Loaded indicator config from {config_path}\")\n",
        "        else:\n",
        "            with open(config_path, 'w') as f:\n",
        "                json.dump(default_config, f, indent=4)\n",
        "            config = default_config\n",
        "            logging.info(f\"Cell 14: Created default indicator config at {config_path}\")\n",
        "        return config\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 14: Failed to load/create config: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "def apply_indicator_config(data, config, is_intraday=False):\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        selected_features = [name for name, params in config['indicators'].items() if params['enabled'] and (is_intraday or not params.get('intraday', False))]\n",
        "\n",
        "        def compute_indicators(group, ticker):\n",
        "            min_rows = 5 if is_intraday else 15\n",
        "            if len(group) < min_rows:\n",
        "                logging.warning(f\"Cell 14: Skipping {ticker}: insufficient data ({len(group)} rows)\")\n",
        "                return None\n",
        "\n",
        "            group = group.set_index('Date')\n",
        "            group = group[~group.index.duplicated(keep='last')]\n",
        "            group['Volume'] = np.log1p(group['Volume'].clip(lower=1))\n",
        "\n",
        "            for indicator, params in config['indicators'].items():\n",
        "                if not params['enabled'] or (params.get('intraday', False) and not is_intraday):\n",
        "                    continue\n",
        "                if indicator == 'RSI_lag1':\n",
        "                    rsi = ta.rsi(group['Close'], length=params['params']['length']).fillna(0)\n",
        "                    group[indicator] = rsi.shift(params['params']['shift']).fillna(0)\n",
        "                elif indicator == 'Volatility_5':\n",
        "                    group[indicator] = group['Close'].pct_change().rolling(window=params['params']['window']).std().fillna(0).clip(upper=0.1)\n",
        "                elif indicator == 'CMF':\n",
        "                    group[indicator] = ta.cmf(group['High'], group['Low'], group['Close'], group['Volume'], length=params['params']['length']).fillna(0)\n",
        "                elif indicator == 'ADX':\n",
        "                    adx_data = ta.adx(group['High'], group['Low'], group['Close'], length=params['params']['length']).fillna(0)\n",
        "                    group[indicator] = adx_data[f'ADX_{params[\"params\"][\"length\"]}'].fillna(0)\n",
        "                elif indicator in ['MACD', 'Signal', 'MACD_Histogram']:\n",
        "                    macd_data = ta.macd(group['Close'], fast=params['params']['fast'], slow=params['params']['slow'], signal=params['params']['signal']).fillna(0)\n",
        "                    if indicator == 'MACD':\n",
        "                        group[indicator] = macd_data[f'MACD_{params[\"params\"][\"fast\"]}_{params[\"params\"][\"slow\"]}_{params[\"params\"][\"signal\"]}'].fillna(0)\n",
        "                    elif indicator == 'Signal':\n",
        "                        group[indicator] = macd_data[f'MACDs_{params[\"params\"][\"fast\"]}_{params[\"params\"][\"slow\"]}_{params[\"params\"][\"signal\"]}'].fillna(0)\n",
        "                    elif indicator == 'MACD_Histogram':\n",
        "                        group[indicator] = macd_data[f'MACDh_{params[\"params\"][\"fast\"]}_{params[\"params\"][\"slow\"]}_{params[\"params\"][\"signal\"]}'].fillna(0)\n",
        "                elif indicator == 'RSI':\n",
        "                    group[indicator] = ta.rsi(group['Close'], length=params['params']['length']).fillna(0)\n",
        "                elif indicator == 'OBV':\n",
        "                    group[indicator] = ta.obv(group['Close'], group['Volume']).fillna(0)\n",
        "                elif indicator == 'ATR':\n",
        "                    group[indicator] = ta.atr(group['High'], group['Low'], group['Close'], length=params['params']['length']).fillna(0)\n",
        "                elif indicator == 'Volume_MA5':\n",
        "                    group[indicator] = ta.sma(group['Volume'], length=params['params']['length']).fillna(0)\n",
        "                elif indicator == 'RSI_M':\n",
        "                    group[indicator] = ta.rsi(group['Close'], length=params['params']['length']).diff().fillna(0)\n",
        "                elif indicator == 'Stoch_RSI':\n",
        "                    stoch_rsi = ta.stochrsi(group['Close'], length=params['params']['length'], k=params['params']['smooth_k'], d=params['params']['smooth_d']).fillna(0)\n",
        "                    group[indicator] = stoch_rsi[f'STOCHRSIk_{params[\"params\"][\"length\"]}_{params[\"params\"][\"smooth_k\"]}_{params[\"params\"][\"smooth_d\"]}'].fillna(0)\n",
        "                elif indicator == 'Volume_Oscillator':\n",
        "                    group[indicator] = (ta.sma(group['Volume'], length=params['params']['short_length']) - ta.sma(group['Volume'], length=params['params']['long_length'])).fillna(0)\n",
        "                elif indicator == 'KC_Upper':\n",
        "                    try:\n",
        "                        kc_data = ta.kc(group['High'], group['Low'], group['Close'], length=params['params']['length'], scalar=params['params']['scalar']).fillna(0)\n",
        "                        logging.info(f\"Cell 14: Keltner Channel columns for {ticker}: {list(kc_data.columns)}\")\n",
        "                        kc_upper_col = next((col for col in kc_data.columns if 'KC' in col and ('U' in col or 'upper' in col.lower())), None)\n",
        "                        if kc_upper_col:\n",
        "                            group[indicator] = kc_data[kc_upper_col].fillna(0)\n",
        "                            logging.info(f\"Cell 14: Using Keltner Channel column {kc_upper_col} for {ticker}\")\n",
        "                        else:\n",
        "                            logging.warning(f\"Cell 14: Keltner Channel upper band not found for {ticker}, computing manually\")\n",
        "                            ema_20 = ta.ema(group['Close'], length=params['params']['length']).fillna(0)\n",
        "                            atr = ta.atr(group['High'], group['Low'], group['Close'], length=14).fillna(1.0)\n",
        "                            group[indicator] = ema_20 + params['params']['scalar'] * atr\n",
        "                            logging.info(f\"Cell 14: Manually computed KC_Upper for {ticker}\")\n",
        "                    except Exception as e:\n",
        "                        logging.warning(f\"Cell 14: Keltner Channel calculation failed for {ticker}: {str(e)}, setting to 0\")\n",
        "                        group[indicator] = 0\n",
        "\n",
        "            indicators = pd.DataFrame({\n",
        "                'Date': group.index,\n",
        "                'Ticker': np.full(len(group), ticker),\n",
        "                'Open': group['Open'],\n",
        "                'High': group['High'],\n",
        "                'Low': group['Low'],\n",
        "                'Close': group['Close'],\n",
        "                'Volume': group['Volume'],\n",
        "                **{col: group[col] for col in selected_features}\n",
        "            })\n",
        "            return indicators\n",
        "\n",
        "        df = data.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        df = df.dropna(subset=['Date']).sort_values(['Ticker', 'Date'])\n",
        "        df = df.drop_duplicates(subset=['Date', 'Ticker'], keep='last')\n",
        "\n",
        "        results = Parallel(n_jobs=-1)(\n",
        "            delayed(compute_indicators)(df[df['Ticker'] == ticker], ticker)\n",
        "            for ticker in df['Ticker'].unique()\n",
        "        )\n",
        "        results = [r for r in results if r is not None]\n",
        "\n",
        "        if not results:\n",
        "            raise ValueError(\"Cell 14: No tickers had sufficient data for indicators\")\n",
        "\n",
        "        df = pd.concat(results, ignore_index=True)\n",
        "        df['Date'] = pd.to_datetime(df['Date'], utc=True)\n",
        "        df = df.dropna().drop_duplicates(subset=['Date', 'Ticker'], keep='last').reset_index(drop=True)\n",
        "\n",
        "        nan_counts = df.isna().sum()\n",
        "        if nan_counts.sum() > 0:\n",
        "            logging.warning(f\"Cell 14: NaN values in indicators: {nan_counts[nan_counts > 0].to_dict()}\")\n",
        "\n",
        "        df.to_csv(f\"{CONFIG['export_dir']}/modular_processed_stock_data.csv\", index=False)\n",
        "        logging.info(f\"Cell 14: Modular indicators calculated, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 14: Modular indicators calculated, Shape: {df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return df\n",
        "\n",
        "def debug_pipeline():\n",
        "    start_time = time.time()\n",
        "    logging.info(\"Cell 14: Starting modular debugging\")\n",
        "    try:\n",
        "        processed_data = pd.read_csv(f\"{CONFIG['export_dir']}/processed_stock_data.csv\")\n",
        "        processed_data = processed_data[processed_data['Ticker'].isin(CONFIG['tickers'])]\n",
        "        processed_data['Date'] = pd.to_datetime(processed_data['Date'], utc=True)\n",
        "        logging.info(f\"Cell 14: Loaded processed data, Shape: {processed_data.shape}\")\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/indicator_weights.db\")\n",
        "        weights_df = pd.read_sql_query(\"SELECT Feature, Weight FROM indicator_weights WHERE Type='Individual' ORDER BY Weight DESC LIMIT 10\", conn)\n",
        "        conn.close()\n",
        "        features = weights_df['Feature'].tolist()\n",
        "        target_multipliers = [0.7, 0.8, 0.9]\n",
        "        stop_loss_multipliers = [1.8, 2.0, 2.2]\n",
        "        feature_combinations = [features]\n",
        "        logging.info(f\"Cell 14: Testing {len(feature_combinations)} feature combinations\")\n",
        "\n",
        "        conn = sqlite3.connect(f\"{CONFIG['export_dir']}/debug_results.db\")\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS debug_results (\n",
        "                Run_ID INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "                Features TEXT,\n",
        "                Win_Rate REAL,\n",
        "                Trades INTEGER,\n",
        "                Signals INTEGER,\n",
        "                Confidence_Mean REAL,\n",
        "                Run_Time REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        results = []\n",
        "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        for combo in feature_combinations:\n",
        "            for tm in target_multipliers:\n",
        "                for slm in stop_loss_multipliers:\n",
        "                    combo_start = time.time()\n",
        "                    logging.info(f\"Cell 14: Testing combo {combo}, target_multiplier={tm}, stop_loss_multiplier={slm}\")\n",
        "\n",
        "                    config = load_indicator_config()\n",
        "                    for ind in config['indicators']:\n",
        "                        config['indicators'][ind]['enabled'] = ind in combo or ind in [f\"{c}_5m\" for c in combo]\n",
        "                    config_path = f\"{CONFIG['export_dir']}/temp_config.json\"\n",
        "                    with open(config_path, 'w') as f:\n",
        "                        json.dump(config, f, indent=4)\n",
        "\n",
        "                    accuracies = []\n",
        "                    for train_idx, test_idx in kf.split(processed_data):\n",
        "                        train_data = processed_data.iloc[train_idx]\n",
        "                        test_data = processed_data.iloc[test_idx]\n",
        "                        transformed_data = fetch_and_transform_data(CONFIG['tickers'])\n",
        "                        processed_train = apply_indicator_config(train_data, config)\n",
        "                        scaler, selected_features, predictions_df = train_model(processed_train)\n",
        "                        trade_candidates_df = generate_trade_candidates(predictions_df)\n",
        "                        backtest_results = backtest_strategy(processed_train, trade_candidates_df, target_multiplier=tm, stop_loss_multiplier=slm)\n",
        "                        intraday_data = fetch_intraday_data(CONFIG['tickers'])\n",
        "                        last_alert_time = monitor_intraday(intraday_data, None) if not intraday_data.empty else None\n",
        "\n",
        "                        win_rate = (backtest_results['Outcome'] == 'Win').mean() * 100 if not backtest_results.empty else 0\n",
        "                        accuracies.append(win_rate)\n",
        "                        logging.info(f\"Cell 14: Fold win rate for {combo}: {win_rate:.2f}\")\n",
        "\n",
        "                    win_rate = np.mean(accuracies) * 100\n",
        "                    signals = len(pd.read_csv(f\"{CONFIG['export_dir']}/intraday_signals.csv\")) if os.path.exists(f\"{CONFIG['export_dir']}/intraday_signals.csv\") else 0\n",
        "                    confidence_mean = predictions_df['Confidence'].mean() if not predictions_df.empty else 0\n",
        "                    results.append({\n",
        "                        'Features': ','.join(combo),\n",
        "                        'Win_Rate': win_rate,\n",
        "                        'Trades': len(backtest_results),\n",
        "                        'Signals': signals,\n",
        "                        'Confidence_Mean': confidence_mean,\n",
        "                        'Run_Time': time.time() - combo_start\n",
        "                    })\n",
        "                    cursor.execute(\n",
        "                        \"INSERT INTO debug_results (Features, Win_Rate, Trades, Signals, Confidence_Mean, Run_Time) VALUES (?, ?, ?, ?, ?, ?)\",\n",
        "                        (','.join(combo), win_rate, len(backtest_results), signals, confidence_mean, time.time() - combo_start)\n",
        "                    )\n",
        "                    conn.commit()\n",
        "\n",
        "        conn.close()\n",
        "        results_df = pd.DataFrame(results)\n",
        "        results_df.to_csv(f\"{CONFIG['export_dir']}/debug_results.csv\", index=False)\n",
        "        logging.info(f\"Cell 14: Debugging completed, Results shape: {results_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        print(f\"Cell 14: Debugging completed, Results shape: {results_df.shape}, Time: {time.time() - start_time:.2f}s\")\n",
        "        return results_df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 14: Failed: {str(e)}\")\n",
        "        print(f\"Cell 14: Failed: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Execute\n",
        "indicator_config = load_indicator_config()\n",
        "debug_results = debug_pipeline()"
      ],
      "metadata": {
        "id": "LADwpMjt_4kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: One-time Historical Indicator Weighting\n",
        "# Purpose: Drop and recreate indicator_weights table, compute feature importance for historical data (2015-07-04 to 2025-07-04) using Random Forest, store weights in SQLite.\n",
        "# Inputs: Historical data from yfinance, indicators from Cell 3.\n",
        "# Outputs: indicator_weights.db updated with feature importance.\n",
        "\n",
        "# Cell 15: One-time Historical Indicator Weighting\n",
        "# Purpose: Drop and recreate indicator_weights table, compute feature importance for historical data (2015-07-04 to 2025-07-04) using Random Forest, store weights in SQLite.\n",
        "# Inputs: Historical data from yfinance, indicators from Cell 3.\n",
        "# Outputs: indicator_weights.db updated with feature importance.\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from retrying import retry\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(filename='pipeline.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Define tickers and date range\n",
        "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'TSLA', 'NVDA', 'PLTR', 'AMD', 'AMZN', 'META', 'INTC',\n",
        "           'SPY', 'QQQ', 'NFLX', 'BA', 'JPM', 'V', 'PYPL', 'DIS', 'ADBE', 'CRM',\n",
        "           'CSCO', 'WMT', 'T', 'VZ', 'CMCSA', 'PFE', 'MRK', 'KO', 'PEP']\n",
        "START_DATE = '2015-07-04'\n",
        "END_DATE = datetime.now().strftime('%Y-%m-%d')\n",
        "\n",
        "@retry(stop_max_attempt_number=3, wait_fixed=2000)\n",
        "def fetch_historical_data(ticker):\n",
        "    \"\"\"Fetch historical data with retry logic.\"\"\"\n",
        "    try:\n",
        "        df = yf.download(ticker, start=START_DATE, end=END_DATE, progress=False)\n",
        "        if df.empty:\n",
        "            logging.warning(f\"No data fetched for {ticker}\")\n",
        "            return None\n",
        "        df['Ticker'] = ticker\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching data for {ticker}: {e}\")\n",
        "        raise\n",
        "\n",
        "def calculate_indicators(df):\n",
        "    \"\"\"Calculate technical indicators using pandas_ta.\"\"\"\n",
        "    try:\n",
        "        df['RSI'] = ta.rsi(df['Close'], length=14)\n",
        "        macd = ta.macd(df['Close'])\n",
        "        df['MACD'] = macd['MACD_12_26_9']\n",
        "        bb = ta.bbands(df['Close'])\n",
        "        df['BB_Upper'] = bb['BBU_5_2.0']\n",
        "        df['ATR'] = ta.atr(df['High'], df['Low'], df['Close'])\n",
        "        df['VWAP'] = ta.vwap(df['High'], df['Low'], df['Close'], df['Volume'])\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error calculating indicators: {e}\")\n",
        "        return None\n",
        "\n",
        "def compute_feature_importance(df):\n",
        "    \"\"\"Compute feature importance using Random Forest.\"\"\"\n",
        "    try:\n",
        "        features = ['RSI', 'MACD', 'BB_Upper', 'ATR', 'VWAP']\n",
        "        df = df.dropna(subset=features)\n",
        "        if df.empty:\n",
        "            logging.warning(\"No valid data for feature importance\")\n",
        "            return None\n",
        "\n",
        "        # Define target: 1 if next day's close > current close by 1%, else 0\n",
        "        df['Target'] = (df['Close'].shift(-1) > df['Close'] * 1.01).astype(int)\n",
        "        df = df.dropna()\n",
        "\n",
        "        X = df[features]\n",
        "        y = df['Target']\n",
        "\n",
        "        model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        model.fit(X, y)\n",
        "\n",
        "        importance = pd.DataFrame({\n",
        "            'Feature': features,\n",
        "            'Importance': model.feature_importances_\n",
        "        })\n",
        "        return importance\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error computing feature importance: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to process tickers and store weights.\"\"\"\n",
        "    try:\n",
        "        # Connect to SQLite database\n",
        "        conn = sqlite3.connect('indicator_weights.db')\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Drop and recreate indicator_weights table\n",
        "        cursor.execute(\"DROP TABLE IF EXISTS indicator_weights\")\n",
        "        cursor.execute(\"\"\"\n",
        "            CREATE TABLE indicator_weights (\n",
        "                ticker TEXT,\n",
        "                feature TEXT,\n",
        "                importance REAL,\n",
        "                date TEXT\n",
        "            )\n",
        "        \"\"\")\n",
        "\n",
        "        # Process each ticker\n",
        "        for ticker in TICKERS:\n",
        "            logging.info(f\"Processing {ticker} for indicator weighting\")\n",
        "\n",
        "            # Fetch and process data\n",
        "            df = fetch_historical_data(ticker)\n",
        "            if df is None:\n",
        "                continue\n",
        "\n",
        "            df = calculate_indicators(df)\n",
        "            if df is None:\n",
        "                continue\n",
        "\n",
        "            importance = compute_feature_importance(df)\n",
        "            if importance is None:\n",
        "                continue\n",
        "\n",
        "            # Store weights in SQLite\n",
        "            current_date = datetime.now().strftime('%Y-%m-%d')\n",
        "            for _, row in importance.iterrows():\n",
        "                cursor.execute(\"\"\"\n",
        "                    INSERT INTO indicator_weights (ticker, feature, importance, date)\n",
        "                    VALUES (?, ?, ?, ?)\n",
        "                \"\"\", (ticker, row['Feature'], row['Importance'], current_date))\n",
        "\n",
        "            conn.commit()\n",
        "            logging.info(f\"Stored weights for {ticker}\")\n",
        "\n",
        "        conn.close()\n",
        "        logging.info(\"Cell 15 completed successfully\")\n",
        "\n",
        "        # Output summary\n",
        "        print(\"Indicator weights calculated and stored in indicator_weights.db\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 15 failed: {e}\")\n",
        "        print(f\"Error in Cell 15: {e}\")\n",
        "        raise\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DBHRzqj7C8X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15Z: Debug Cell 15 Historical Indicator Weighting\n",
        "# Purpose: Test data fetching, indicator calculation, and SQLite storage for one ticker to identify errors.\n",
        "# Inputs: AAPL historical data.\n",
        "# Outputs: Debug logs, sample weights printed.\n",
        "\n",
        "# Cell 15Z: Debug Cell 15 Historical Indicator Weighting\n",
        "# Purpose: Test data fetching, indicator calculation, and SQLite storage for one ticker to identify errors.\n",
        "# Inputs: AAPL historical data.\n",
        "# Outputs: Debug logs, sample weights printed.\n",
        "\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import pandas_ta as ta\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(filename='pipeline.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def debug_cell_15():\n",
        "    try:\n",
        "        # Test data fetching\n",
        "        ticker = 'AAPL'\n",
        "        df = yf.download(ticker, start='2015-07-04', end=datetime.now().strftime('%Y-%m-%d'), progress=False)\n",
        "        logging.debug(f\"Data shape for {ticker}: {df.shape}\")\n",
        "        print(f\"Data shape: {df.shape}\")\n",
        "        if df.empty:\n",
        "            logging.error(\"Empty DataFrame\")\n",
        "            return\n",
        "\n",
        "        # Test indicator calculation\n",
        "        df['RSI'] = ta.rsi(df['Close'], length=14)\n",
        "        logging.debug(f\"RSI sample: {df['RSI'].iloc[-5:]}\")\n",
        "        print(f\"RSI sample:\\n{df['RSI'].iloc[-5:]}\")\n",
        "\n",
        "        # Test SQLite connection\n",
        "        conn = sqlite3.connect('indicator_weights.db')\n",
        "        cursor = conn.cursor()\n",
        "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n",
        "        tables = cursor.fetchall()\n",
        "        logging.debug(f\"Database tables: {tables}\")\n",
        "        print(f\"Database tables: {tables}\")\n",
        "\n",
        "        conn.close()\n",
        "        logging.info(\"Cell 15Z debug completed\")\n",
        "        print(\"Debug completed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Cell 15Z failed: {e}\")\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    debug_cell_15()"
      ],
      "metadata": {
        "id": "lPsG9pjtoyQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: CLI Interface\n",
        "import argparse\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Trading Pipeline\")\n",
        "    parser.add_argument('--tickers', nargs='+', default=CONFIG['tickers'], help=\"List of tickers to process\")\n",
        "    args = parser.parse_args()\n",
        "    CONFIG['tickers'] = args.tickers\n",
        "    run_pipeline()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "r7-_NBhE3n54"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5xg6QXKQ/DP4tgH4Ec96V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}